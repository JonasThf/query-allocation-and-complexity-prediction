{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afd8d57",
   "metadata": {},
   "source": [
    "# LLM-Performance-Based Routing Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885aade9",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "457b0074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model for t-threshold: t0 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t0 Trained in 83.1312 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t0_p80_rf_nest200.pkl\n",
      "\n",
      "--- Training Model for t-threshold: t-0.005 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t-0.005 Trained in 97.6311 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t-0.005_p80_rf_nest200.pkl\n",
      "\n",
      "--- Training Model for t-threshold: t-0.01 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t-0.01 Trained in 116.0043 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t-0.01_p80_rf_nest200.pkl\n",
      "\n",
      "--- Training Model for t-threshold: t-0.015 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t-0.015 Trained in 91.3852 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t-0.015_p80_rf_nest200.pkl\n",
      "\n",
      "--- Training Model for t-threshold: t-0.02 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t-0.02 Trained in 122.8367 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t-0.02_p80_rf_nest200.pkl\n",
      "\n",
      "--- Training Model for t-threshold: t-0.025 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t-0.025 Trained in 107.1397 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t-0.025_p80_rf_nest200.pkl\n",
      "\n",
      "--- Training Model for t-threshold: t-0.03 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Evaluation Model for t-0.03 Trained in 138.9083 seconds.\n",
      "Saved as: approach_2_pretrained_models/evaluation_model_t-0.03_p80_rf_nest200.pkl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configuration\n",
    "exclude_percentile = 80  # Removes the top X% most complex queries\n",
    "train_ratio = 0.90       # 90% Training, 10% Testing\n",
    "\n",
    "# Parameters for training multiple models on different t-thresholds\n",
    "file_names = [\n",
    "    (\"t0\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt0_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.005\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.005_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.01\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.01_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.015\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.015_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.02\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.02_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.025\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.025_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.03\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.03_tt3_rt3.jsonl\"),\n",
    "]\n",
    "\n",
    "# Loop over each dataset\n",
    "for t_threshold, file_path in file_names:\n",
    "    print(f\"\\n--- Training Model for t-threshold: {t_threshold} ---\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_json(file_path, orient='records', lines=True)\n",
    "\n",
    "    # Filter out the top X% most complex queries\n",
    "    complexity_threshold = df['query_complexity_score'].quantile(exclude_percentile / 100)\n",
    "    filtered_df = df[df['query_complexity_score'] <= complexity_threshold].copy()\n",
    "    num_removed = len(df) - len(filtered_df)\n",
    "\n",
    "    print(f\"Filtered out the top {100 - exclude_percentile}% most complex queries.\")\n",
    "    print(f\"Total queries removed: {num_removed}. Remaining for training/testing: {len(filtered_df)}.\")\n",
    "\n",
    "    # Compute dynamic train-test split\n",
    "    num_train = round(len(filtered_df) * train_ratio)  # 90% Training\n",
    "    num_test = len(filtered_df) - num_train            # 10% Testing\n",
    "\n",
    "    train_df = filtered_df.iloc[:num_train].copy()     # Training Data\n",
    "\n",
    "    print(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\")\n",
    "\n",
    "    # Keep only relevant columns for training\n",
    "    train_df = train_df[['id', 'query', 'instruction', 'input', \n",
    "                         'query_chars_count', 'query_words_count', 'query_unique_word_count', \n",
    "                         'query_readability_score', 'query_special_tokens_count', \n",
    "                         'query_keywords_count', 'query_contains_url', 'evaluation_model_label']]\n",
    "\n",
    "    # Extract X and Y for training\n",
    "    X_train_evaluation = train_df[['query', 'query_chars_count', 'query_words_count', 'query_unique_word_count', \n",
    "                                 'query_readability_score', 'query_special_tokens_count', 'query_keywords_count',\n",
    "                                 'query_contains_url']]\n",
    "    y_train_evaluation = train_df['evaluation_model_label']\n",
    "\n",
    "    # Define model pipeline\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000), 'query'),\n",
    "            ('scaler', StandardScaler(), [\n",
    "                'query_chars_count', 'query_words_count', 'query_unique_word_count',\n",
    "                'query_readability_score', 'query_special_tokens_count', 'query_keywords_count', 'query_contains_url'\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('transformer', column_transformer),\n",
    "        ('clf', RandomForestClassifier(n_estimators=200, max_depth=None, n_jobs=-1, class_weight='balanced', random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train_evaluation, y_train_evaluation)\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "\n",
    "    # Save model with matching name (t-threshold & percentile)\n",
    "    model_filename = f\"trained_models/evaluation_model_{t_threshold}_p{exclude_percentile}_rf_nest200.pkl\"\n",
    "    joblib.dump(pipeline, model_filename)\n",
    "    \n",
    "    print(f\"Evaluation Model for {t_threshold} Trained in {train_time:.4f} seconds.\")\n",
    "    print(f\"Saved as: {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c6870",
   "metadata": {},
   "source": [
    "### Model Testing for Feature Calculation + Model Label Prediction for all Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d178f34e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Model for t-threshold: t0 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.9887 seconds.\n",
      "Model Prediction Completed in 0.3862 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.3749 seconds\n",
      "Avg Time per Query: 0.000594 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 69.42%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.15      0.22       552\n",
      "           2       0.59      0.21      0.31       754\n",
      "           3       0.72      0.94      0.82      2694\n",
      "\n",
      "    accuracy                           0.69      4000\n",
      "   macro avg       0.57      0.43      0.45      4000\n",
      "weighted avg       0.65      0.69      0.64      4000\n",
      "\n",
      "Saved predictions for t0 to: predicted_model_label/predicted_model_label_t0.jsonl\n",
      "\n",
      "--- Testing Model for t-threshold: t-0.005 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.7918 seconds.\n",
      "Model Prediction Completed in 0.6007 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.3925 seconds\n",
      "Avg Time per Query: 0.000598 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 66.95%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.23      0.31       630\n",
      "           2       0.56      0.22      0.31       804\n",
      "           3       0.70      0.92      0.79      2566\n",
      "\n",
      "    accuracy                           0.67      4000\n",
      "   macro avg       0.58      0.46      0.47      4000\n",
      "weighted avg       0.63      0.67      0.62      4000\n",
      "\n",
      "Saved predictions for t-0.005 to: predicted_model_label/predicted_model_label_t-0.005.jsonl\n",
      "\n",
      "--- Testing Model for t-threshold: t-0.01 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.8473 seconds.\n",
      "Model Prediction Completed in 0.3799 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.2272 seconds\n",
      "Avg Time per Query: 0.000557 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 64.65%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.29      0.36       727\n",
      "           2       0.56      0.20      0.30       845\n",
      "           3       0.68      0.91      0.77      2428\n",
      "\n",
      "    accuracy                           0.65      4000\n",
      "   macro avg       0.57      0.47      0.48      4000\n",
      "weighted avg       0.62      0.65      0.60      4000\n",
      "\n",
      "Saved predictions for t-0.01 to: predicted_model_label/predicted_model_label_t-0.01.jsonl\n",
      "\n",
      "--- Testing Model for t-threshold: t-0.015 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.9158 seconds.\n",
      "Model Prediction Completed in 0.3744 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.2902 seconds\n",
      "Avg Time per Query: 0.000573 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 62.42%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.37      0.43       826\n",
      "           2       0.57      0.20      0.30       896\n",
      "           3       0.65      0.88      0.75      2278\n",
      "\n",
      "    accuracy                           0.62      4000\n",
      "   macro avg       0.58      0.48      0.49      4000\n",
      "weighted avg       0.60      0.62      0.58      4000\n",
      "\n",
      "Saved predictions for t-0.015 to: predicted_model_label/predicted_model_label_t-0.015.jsonl\n",
      "\n",
      "--- Testing Model for t-threshold: t-0.02 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.8373 seconds.\n",
      "Model Prediction Completed in 0.3832 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.2205 seconds\n",
      "Avg Time per Query: 0.000555 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 60.32%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.43      0.48       951\n",
      "           2       0.57      0.21      0.31       913\n",
      "           3       0.62      0.85      0.72      2136\n",
      "\n",
      "    accuracy                           0.60      4000\n",
      "   macro avg       0.58      0.50      0.50      4000\n",
      "weighted avg       0.59      0.60      0.57      4000\n",
      "\n",
      "Saved predictions for t-0.02 to: predicted_model_label/predicted_model_label_t-0.02.jsonl\n",
      "\n",
      "--- Testing Model for t-threshold: t-0.025 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.9401 seconds.\n",
      "Model Prediction Completed in 0.3934 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.3335 seconds\n",
      "Avg Time per Query: 0.000583 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 58.35%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.46      0.49      1074\n",
      "           2       0.55      0.23      0.32       905\n",
      "           3       0.61      0.81      0.69      2021\n",
      "\n",
      "    accuracy                           0.58      4000\n",
      "   macro avg       0.56      0.50      0.50      4000\n",
      "weighted avg       0.57      0.58      0.56      4000\n",
      "\n",
      "Saved predictions for t-0.025 to: predicted_model_label/predicted_model_label_t-0.025.jsonl\n",
      "\n",
      "--- Testing Model for t-threshold: t-0.03 ---\n",
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Query Features Computed in 1.9048 seconds.\n",
      "Model Prediction Completed in 0.3797 seconds.\n",
      "Total Feature Extraction & Prediction Time: 2.2844 seconds\n",
      "Avg Time per Query: 0.000571 seconds\n",
      "\n",
      "EVALUATION MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 56.47%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.50      0.51      1189\n",
      "           2       0.54      0.24      0.33       897\n",
      "           3       0.58      0.76      0.66      1914\n",
      "\n",
      "    accuracy                           0.56      4000\n",
      "   macro avg       0.55      0.50      0.50      4000\n",
      "weighted avg       0.56      0.56      0.54      4000\n",
      "\n",
      "Saved predictions for t-0.03 to: predicted_model_label/predicted_model_label_t-0.03.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import joblib\n",
    "from textstat import textstat\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "exclude_percentile = 80  # Exclude top X% most complex queries\n",
    "train_ratio = 0.90       # 90% Training, 10% Testing\n",
    "output_text_file = \"discrepancy_thresholds_testing_results.txt\"\n",
    "\n",
    "# Dataset & model pairs\n",
    "evaluation_files = [\n",
    "    (\"t0\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt0_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.005\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.005_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.01\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.01_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.015\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.015_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.02\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.02_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.025\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.025_tt3_rt3.jsonl\"),\n",
    "    (\"t-0.03\", \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.03_tt3_rt3.jsonl\"),\n",
    "]\n",
    "\n",
    "trained_models = [\n",
    "    (\"t0\", \"trained_models/evaluation_model_t0_p80_rf_nest200.pkl\"),\n",
    "    (\"t-0.005\", \"trained_models/evaluation_model_t-0.005_p80_rf_nest200.pkl\"),\n",
    "    (\"t-0.01\", \"trained_models/evaluation_model_t-0.01_p80_rf_nest200.pkl\"),\n",
    "    (\"t-0.015\", \"trained_models/evaluation_model_t-0.015_p80_rf_nest200.pkl\"),\n",
    "    (\"t-0.02\", \"trained_models/evaluation_model_t-0.02_p80_rf_nest200.pkl\"),\n",
    "    (\"t-0.025\", \"trained_models/evaluation_model_t-0.025_p80_rf_nest200.pkl\"),\n",
    "    (\"t-0.03\", \"trained_models/evaluation_model_t-0.03_p80_rf_nest200.pkl\"),\n",
    "]\n",
    "\n",
    "# Open text file to store results\n",
    "with open(output_text_file, \"w\") as result_file:\n",
    "\n",
    "    # Iterate through each dataset & corresponding trained model\n",
    "    for (t_threshold, dataset_path), (_, model_path) in zip(evaluation_files, trained_models):\n",
    "        print(f\"\\n--- Testing Model for t-threshold: {t_threshold} ---\")\n",
    "        result_file.write(f\"\\n--- Testing Model for t-threshold: {t_threshold} ---\\n\")\n",
    "\n",
    "        # Load correct dataset\n",
    "        df = pd.read_json(dataset_path, orient='records', lines=True)\n",
    "\n",
    "        # Determine complexity threshold\n",
    "        complexity_threshold = df['query_complexity_score'].quantile(exclude_percentile / 100)\n",
    "\n",
    "        # Filter out the top X% most complex queries\n",
    "        filtered_df = df[df['query_complexity_score'] <= complexity_threshold].copy()\n",
    "        num_removed = len(df) - len(filtered_df)\n",
    "\n",
    "        print(f\"Filtered out the top {100 - exclude_percentile}% most complex queries.\")\n",
    "        print(f\"Total queries removed: {num_removed}. Remaining for training/testing: {len(filtered_df)}.\")\n",
    "\n",
    "        # Compute Train-Test Split\n",
    "        num_train = round(len(filtered_df) * train_ratio)\n",
    "        num_test = len(filtered_df) - num_train\n",
    "\n",
    "        train_df = filtered_df.iloc[:num_train].copy()\n",
    "        test_df = filtered_df.iloc[num_train:].copy()\n",
    "\n",
    "        print(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\")\n",
    "        result_file.write(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\\n\")\n",
    "        \n",
    "        # Load the correct model\n",
    "        pipeline_evaluation = joblib.load(model_path)\n",
    "        \n",
    "        # Prepare testing data\n",
    "        test_df = test_df[['id', 'query', 'instruction', 'input']]\n",
    "        print(f\"Loaded {len(test_df)} queries for testing.\")\n",
    "\n",
    "        def calculate_query_features(df):\n",
    "            \"\"\"\n",
    "            Extracts features from queries, such as length, special tokens, keywords, and readability.\n",
    "\n",
    "            Args:\n",
    "                df (pd.DataFrame): DataFrame containing query data.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Updated DataFrame with additional query-based features.\n",
    "            \"\"\"\n",
    "            special_tokens = [\":\", \";\", \"=\", \"+\", \"-\", \"_\", \"/\", \".\", \"'\", '\"', \"´\", \"`\", \",\", \"<\", \">\", \"[\", \"]\", \"{\", \"}\", \"(\", \")\", \"?\", \"!\", \"*\", \"&\", \"$\", \"#\", \"@\", \"%\", \"^\", \"~\", \"|\", \"\\\\\"]\n",
    "            keywords = [\n",
    "                \"analyze\", \"synthesize\", \"interpret\", \"evaluate\", \"justify\", \"compare\", \n",
    "                \"optimize\", \"hypothesize\", \"formulate\", \"simulate\", \"derive\", \"describe\",\n",
    "                \"validate\", \"correlate\", \"quantify\", \"investigate\", \"predict\", \"forecast\",\n",
    "                \"prove\", \"assess\", \"criticize\", \"argue\", \"solve\", \"reconstruct\", \"theorize\",\n",
    "                \"explore\", \"elaborate\", \"deduce\", \"refute\", \"conceptualize\", \"identify\", \"outline\",\n",
    "                \"rationalize\", \"articulate\", \"summarize\", \"innovate\", \"extrapolate\", \"explain\", \"clarify\"\n",
    "            ]\n",
    "\n",
    "            def remove_urls(text):\n",
    "                \"\"\"\n",
    "                Removes URLs and image links from a given text.\n",
    "\n",
    "                Args:\n",
    "                    text (str): The input text.\n",
    "\n",
    "                Returns:\n",
    "                    str: The cleaned text without URLs.\n",
    "                \"\"\"\n",
    "                return re.sub(r'!\\[.*?\\]\\(.*?\\)|http[s]?://\\S+', '', text)\n",
    "\n",
    "            df['query_chars_count'] = df['query'].apply(lambda x: len(str(x)))\n",
    "            df['query_words_count'] = df['query'].apply(lambda x: len(str(x).split()))\n",
    "            df['query_unique_word_count'] = df['query'].apply(lambda x: len(set(str(x).split())))\n",
    "            df['query_readability_score'] = df['query'].apply(lambda x: textstat.flesch_kincaid_grade(remove_urls(x)))\n",
    "            df['query_special_tokens_count'] = df['query'].apply(lambda x: sum(1 for char in str(x) if char in special_tokens))\n",
    "            df['query_keywords_count'] = df.apply(lambda row: (\n",
    "                sum(len(re.findall(r'\\b{}\\b'.format(re.escape(keyword)), str(row.get('instruction', '')).lower())) for keyword in keywords)\n",
    "                if row.get('instruction') \n",
    "                else sum(len(re.findall(r'\\b{}\\b'.format(re.escape(keyword)), str(row.get('input', '')).lower())) for keyword in keywords)\n",
    "                ), axis=1)\n",
    "            df['query_contains_url'] = df['query'].apply(lambda x: int(bool(re.search(r'http[s]?://', str(x)))))\n",
    "\n",
    "            return df\n",
    "\n",
    "        # Apply feature calculation\n",
    "        start_time = time.time()\n",
    "        test_df = calculate_query_features(test_df)\n",
    "        end_time = time.time()\n",
    "        query_features_time = end_time - start_time\n",
    "\n",
    "        print(f\"Query Features Computed in {query_features_time:.4f} seconds.\")\n",
    "        result_file.write(f\"Query Features Computed in {query_features_time:.4f} seconds.\\n\")\n",
    "\n",
    "        # Label prediction\n",
    "        features_to_drop = ['id', 'instruction', 'input']\n",
    "        test_X_evaluation = test_df.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "        start_time = time.time()\n",
    "        y_pred_proba = pipeline_evaluation.predict_proba(test_X_evaluation)  # Get Probabilities\n",
    "        y_pred_evaluation = y_pred_proba.argmax(axis=1) + 1  # Convert to label (1,2,3)\n",
    "        end_time = time.time()\n",
    "        evaluation_test_time = end_time - start_time\n",
    "\n",
    "        print(f\"Model Prediction Completed in {evaluation_test_time:.4f} seconds.\")\n",
    "        result_file.write(f\"Model Prediction Completed in {evaluation_test_time:.4f} seconds.\\n\")\n",
    "\n",
    "        # Store predictions and probabilities\n",
    "        test_df['predicted_model_label'] = y_pred_evaluation\n",
    "        test_df['proba_base'] = y_pred_proba[:, 0]\n",
    "        test_df['proba_large'] = y_pred_proba[:, 1]\n",
    "        test_df['proba_xl'] = y_pred_proba[:, 2]\n",
    "        \n",
    "        # Time summary\n",
    "        total_time = evaluation_test_time + query_features_time\n",
    "        avg_time_per_query = total_time / len(test_df)\n",
    "\n",
    "        print(f\"Total Feature Extraction & Prediction Time: {total_time:.4f} seconds\")\n",
    "        result_file.write(f\"Total Inference Time (All Steps): {total_time:.4f} seconds.\\n\")\n",
    "        print(f\"Avg Time per Query: {avg_time_per_query:.6f} seconds\")\n",
    "        result_file.write(f\"Avg Time per Query: {avg_time_per_query:.6f} seconds.\\n\")\n",
    "\n",
    "        # Restore actual labels for evaluation\n",
    "        test_df['evaluation_model_label'] = filtered_df.iloc[num_train:]['evaluation_model_label'].values\n",
    "\n",
    "        # Model evaluation\n",
    "        evaluation_accuracy = accuracy_score(test_df['evaluation_model_label'], test_df['predicted_model_label'])\n",
    "        evaluation_report = classification_report(test_df['evaluation_model_label'], test_df['predicted_model_label'])\n",
    "\n",
    "        print(\"\\nEVALUATION MODEL EVALUATION\")\n",
    "        result_file.write(\"\\nEVALUATION MODEL EVALUATION\\n\")\n",
    "        print(f\"Model Label Prediction Accuracy: {evaluation_accuracy * 100:.2f}%\")\n",
    "        result_file.write(f\"Model Label Prediction Accuracy: {evaluation_accuracy * 100:.2f}%\\n\")\n",
    "        print(\"\\nClassification Report:\\n\", evaluation_report)\n",
    "        result_file.write(\"\\nClassification Report:\\n\" + evaluation_report + \"\\n\")\n",
    "\n",
    "\n",
    "        # Define output file path\n",
    "        output_file_path = f'predicted_model_label/predicted_model_label_{t_threshold}.jsonl'\n",
    "\n",
    "        # Write DataFrame to a JSONL file including probabilities\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            for entry in test_df[['id', 'predicted_model_label', 'proba_base', 'proba_large', 'proba_xl']].to_dict(orient='records'):\n",
    "                json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "                output_file.write('\\n')\n",
    "\n",
    "        print(f\"Saved predictions for {t_threshold} to: {output_file_path}\")\n",
    "        result_file.write(f\"Saved predictions for {t_threshold} to: {output_file_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe1fd5",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f7a235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed. Results saved to: discrepancy_thresholds_evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "exclude_percentile = 80  # Exclude top X% most complex queries\n",
    "train_ratio = 0.90       # 90% Training, 10% Testing\n",
    "\n",
    "# Define different probability thresholds\n",
    "probability_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Define compute unit consumption for each model\n",
    "COMPUTE_UNITS = {1: 1.0, 2: 3.12, 3: 12.0}\n",
    "\n",
    "# === Define Model Files and Corresponding Datasets ===\n",
    "model_files = [\n",
    "    \"trained_models/evaluation_model_t0_p80_rf_nest200.pkl\",\n",
    "    \"trained_models/evaluation_model_t-0.005_p80_rf_nest200.pkl\",\n",
    "    \"trained_models/evaluation_model_t-0.01_p80_rf_nest200.pkl\",\n",
    "    \"trained_models/evaluation_model_t-0.015_p80_rf_nest200.pkl\",\n",
    "    \"trained_models/evaluation_model_t-0.02_p80_rf_nest200.pkl\",\n",
    "    \"trained_models/evaluation_model_t-0.025_p80_rf_nest200.pkl\",\n",
    "    \"trained_models/evaluation_model_t-0.03_p80_rf_nest200.pkl\",\n",
    "]\n",
    "\n",
    "dataset_files = [\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt0_tt3_rt3.jsonl\",\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.005_tt3_rt3.jsonl\",\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.01_tt3_rt3.jsonl\",\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.015_tt3_rt3.jsonl\",\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.02_tt3_rt3.jsonl\",\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.025_tt3_rt3.jsonl\",\n",
    "    \"../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.03_tt3_rt3.jsonl\",\n",
    "]\n",
    "\n",
    "def confidence_based_routing(df, y_pred_proba, confidence_threshold):\n",
    "    \"\"\"\n",
    "    Routes queries based on confidence scores of model predictions.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing query information.\n",
    "        y_pred_proba (numpy.ndarray): Predicted probabilities for each class.\n",
    "        confidence_threshold (float): Minimum confidence required to accept a prediction.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated dataset with assigned model labels.\n",
    "    \"\"\"\n",
    "    routed_to = []\n",
    "\n",
    "    for probs in y_pred_proba:\n",
    "        max_confidence = max(probs)\n",
    "        predicted_label = probs.argmax() + 1  # Add 1 to match labels (1, 2, 3)\n",
    "\n",
    "        # Use confidence threshold to adjust predictions\n",
    "        if max_confidence < confidence_threshold:\n",
    "            routed_to.append(3)  # Route to XL\n",
    "        else:\n",
    "            routed_to.append(predicted_label)\n",
    "\n",
    "    df['routed_to'] = routed_to\n",
    "    return df\n",
    "\n",
    "def calculate_quality_discrepancies(df):\n",
    "    \"\"\"\n",
    "    Computes the quality loss when routing based on predicted labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with routing results.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (quality_loss_to_gt, quality_loss_to_xl)\n",
    "    \"\"\"\n",
    "    total_score_gt = df.apply(lambda row: row[f\"avg_normalized_score_t5_{['base', 'large', 'xl'][row['evaluation_model_label'] - 1]}\"], axis=1).sum()\n",
    "    total_score_router = df.apply(lambda row: row[f\"avg_normalized_score_t5_{['base', 'large', 'xl'][row['routed_to'] - 1]}\"], axis=1).sum()\n",
    "    total_score_xl = df['avg_normalized_score_t5_xl'].sum()\n",
    "\n",
    "    num_queries = len(df)\n",
    "\n",
    "    avg_score_gt = total_score_gt / num_queries\n",
    "    avg_score_router = total_score_router / num_queries\n",
    "    avg_score_xl = total_score_xl / num_queries\n",
    "\n",
    "    quality_loss_to_gt = ((avg_score_router - avg_score_gt) / avg_score_gt) * 100\n",
    "    quality_loss_to_xl = ((avg_score_router - avg_score_xl) / avg_score_xl) * 100\n",
    "\n",
    "    return quality_loss_to_gt, quality_loss_to_xl\n",
    "\n",
    "# Open file for writing results\n",
    "result_file_path = \"discrepancy_thresholds_evaluation_results.txt\"\n",
    "\n",
    "with open(result_file_path, \"w\") as result_file:\n",
    "    for model_file, dataset_file in zip(model_files, dataset_files):\n",
    "        t_threshold = model_file.split(\"_\")[-4]  # Extract t-threshold\n",
    "\n",
    "        result_file.write(f\"\\n--- Evaluating Model for t-threshold: {t_threshold} ---\\n\")\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_json(dataset_file, orient='records', lines=True)\n",
    "\n",
    "        # Filter out the top X% most complex queries\n",
    "        complexity_threshold = df['query_complexity_score'].quantile(exclude_percentile / 100)\n",
    "        filtered_df = df[df['query_complexity_score'] <= complexity_threshold].copy()\n",
    "\n",
    "        num_removed = len(df) - len(filtered_df)\n",
    "\n",
    "        result_file.write(f\"Filtered out the top {100 - exclude_percentile}% most complex queries.\\n\")\n",
    "\n",
    "        # Split dataset\n",
    "        num_train = round(len(filtered_df) * train_ratio)\n",
    "        num_test = len(filtered_df) - num_train\n",
    "\n",
    "        train_df = filtered_df.iloc[:num_train].copy()\n",
    "        test_df = filtered_df.iloc[num_train:].copy()\n",
    "\n",
    "        result_file.write(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\\n\")\n",
    "\n",
    "        # Load predictions and probabilities\n",
    "        predictions_file = f\"predicted_model_label/predicted_model_label_{t_threshold}.jsonl\"\n",
    "        predicted_labels_df = pd.read_json(predictions_file, orient='records', lines=True)\n",
    "\n",
    "        # Merge with test data\n",
    "        test_df = test_df.merge(predicted_labels_df, on='id', how='left')\n",
    "        \n",
    "        # Check if merge worked\n",
    "        if 'predicted_model_label' not in test_df.columns:\n",
    "            raise KeyError(\"Error: 'predicted_model_label' is missing after merging!\")\n",
    "            \n",
    "        # Compute average maximum probability across all queries\n",
    "        avg_max_proba = test_df[['proba_base', 'proba_large', 'proba_xl']].max(axis=1).mean()\n",
    "\n",
    "        result_file.write(f\"Average Maximum Probability per Query: {avg_max_proba:.4f}\\n\")\n",
    "\n",
    "        # Iterate over probability thresholds\n",
    "        for threshold in probability_thresholds:\n",
    "            result_file.write(f\"\\nEvaluating Probability Threshold: {threshold:.1f}\\n\")\n",
    "\n",
    "            # Measure routing time\n",
    "            start_time = time.time()\n",
    "            test_df = confidence_based_routing(test_df, test_df[['proba_base', 'proba_large', 'proba_xl']].values, threshold)\n",
    "            end_time = time.time()\n",
    "            routing_time = end_time - start_time\n",
    "            avg_routing_time_per_query = routing_time / len(test_df)\n",
    "            \n",
    "            # Count queries routed to each model\n",
    "            num_routed_base = (test_df['routed_to'] == 1).sum()\n",
    "            num_routed_large = (test_df['routed_to'] == 2).sum()\n",
    "            num_routed_xl = (test_df['routed_to'] == 3).sum()\n",
    "            \n",
    "            # Compute metrics\n",
    "            accuracy = accuracy_score(test_df['evaluation_model_label'], test_df['routed_to']) * 100\n",
    "            quality_loss_gt, quality_loss_xl = calculate_quality_discrepancies(test_df)\n",
    "\n",
    "            # Define compute usage and savings\n",
    "            compute_units_gt = sum(test_df['evaluation_model_label'].map(COMPUTE_UNITS))\n",
    "            compute_units_router = sum(test_df['routed_to'].map(COMPUTE_UNITS))\n",
    "            compute_units_xl = len(test_df) * COMPUTE_UNITS[3]\n",
    "\n",
    "            compute_savings_gt = ((compute_units_gt - compute_units_router) / compute_units_gt) * 100\n",
    "            compute_savings_xl = ((compute_units_xl - compute_units_router) / compute_units_xl) * 100\n",
    "\n",
    "            # Print & save results\n",
    "            output = (\n",
    "                f\"Quality Loss compared to GT: {quality_loss_gt:.2f}%\\n\"\n",
    "                f\"Quality Loss compared to XL: {quality_loss_xl:.2f}%\\n\"\n",
    "                f\"Compute Savings compared to GT: {compute_savings_gt:.2f}%\\n\"\n",
    "                f\"Compute Savings compared to XL: {compute_savings_xl:.2f}%\\n\"\n",
    "                f\"Routing Accuracy: {accuracy:.2f}%\\n\"\n",
    "                f\"Total Compute Units Used: {compute_units_router:.2f} CUs\\n\"\n",
    "                f\"Avg Routing Time per Query: {avg_routing_time_per_query:.8f} seconds\\n\"\n",
    "                f\"Number of queries routed to Base: {num_routed_base}\\n\"\n",
    "                f\"Number of queries routed to Large: {num_routed_large}\\n\"\n",
    "                f\"Number of queries routed to XL: {num_routed_xl}\\n\"\n",
    "            )\n",
    "\n",
    "            result_file.write(output)\n",
    "\n",
    "print(f\"\\nEvaluation completed. Results saved to: {result_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cab230",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c24898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define result file path\n",
    "result_file_path = \"discrepancy_thresholds_evaluation_results.txt\"\n",
    "\n",
    "# Initialize lists to store parsed results\n",
    "t_thresholds = []\n",
    "probability_thresholds = []\n",
    "quality_loss_gt = []\n",
    "quality_loss_xl = []\n",
    "compute_savings_gt = []\n",
    "compute_savings_xl = []\n",
    "routing_accuracy = []\n",
    "\n",
    "# Read and parse the result file\n",
    "with open(result_file_path, \"r\") as file:\n",
    "    current_t_threshold = None\n",
    "    current_prob_threshold = None\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detect new t-threshold\n",
    "        if line.startswith(\"--- Evaluating Model for t-threshold: \"):\n",
    "            match = re.search(r\"t-?\\d*\\.?\\d+\", line)\n",
    "            if match:\n",
    "                current_t_threshold = match.group(0)\n",
    "\n",
    "        # Detect probability threshold\n",
    "        elif line.startswith(\"Evaluating Probability Threshold: \"):\n",
    "            current_prob_threshold = float(line.split(\": \")[1])\n",
    "\n",
    "        # Extract metrics\n",
    "        elif line.startswith(\"Quality Loss compared to GT:\"):\n",
    "            quality_loss_gt.append(float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)[0]))\n",
    "        elif line.startswith(\"Quality Loss compared to XL:\"):\n",
    "            quality_loss_xl.append(float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)[0]))\n",
    "        elif line.startswith(\"Compute Savings compared to GT:\"):\n",
    "            compute_savings_gt.append(float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)[0]))\n",
    "        elif line.startswith(\"Compute Savings compared to XL:\"):\n",
    "            compute_savings_xl.append(float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)[0]))\n",
    "        elif line.startswith(\"Routing Accuracy:\"):\n",
    "            routing_accuracy.append(float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)[0]))\n",
    "\n",
    "            # Store the extracted threshold values correctly\n",
    "            t_thresholds.append(current_t_threshold)\n",
    "            probability_thresholds.append(current_prob_threshold)\n",
    "\n",
    "# Create DataFrame for easier plotting\n",
    "df_results = pd.DataFrame({\n",
    "    \"t_threshold\": t_thresholds,\n",
    "    \"prob_threshold\": probability_thresholds,\n",
    "    \"quality_loss_gt\": quality_loss_gt,\n",
    "    \"quality_loss_xl\": quality_loss_xl,\n",
    "    \"compute_savings_gt\": compute_savings_gt,\n",
    "    \"compute_savings_xl\": compute_savings_xl,\n",
    "    \"routing_accuracy\": routing_accuracy\n",
    "})\n",
    "\n",
    "# Define distinct colors for each threshold, avoiding light colors\n",
    "threshold_colors = {\n",
    "    \"t0\": \"blue\",\n",
    "    \"t-0.005\": \"green\",\n",
    "    \"t-0.01\": \"red\",\n",
    "    \"t-0.015\": \"purple\",\n",
    "    \"t-0.02\": \"darkorange\",\n",
    "    \"t-0.025\": \"brown\",\n",
    "    \"t-0.03\": \"dodgerblue\"\n",
    "}\n",
    "\n",
    "# Dictionary of metric names\n",
    "metric_titles = {\n",
    "    \"quality_loss_gt\": \"Quality Loss vs GT\",\n",
    "    \"quality_loss_xl\": \"Quality Loss vs XL\",\n",
    "    \"compute_savings_gt\": \"Compute Savings vs GT\",\n",
    "    \"compute_savings_xl\": \"Compute Savings vs XL\",\n",
    "    \"routing_accuracy\": \"Routing Accuracy\"\n",
    "}\n",
    "\n",
    "# Function to generate a plot for a given metric\n",
    "def plot_metric(metric, ylabel, filename):\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for t in sorted(df_results[\"t_threshold\"].unique()):\n",
    "        subset = df_results[df_results[\"t_threshold\"] == t]\n",
    "        color = threshold_colors.get(t, \"black\")\n",
    "        plt.plot(subset[\"prob_threshold\"], subset[metric], marker=\"o\", label=f\"{t}\", color=color, alpha=1.0)\n",
    "\n",
    "    plt.xlabel(\"Probability Threshold\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save as PDF\n",
    "    plt.savefig(filename, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "# Generate and save plots\n",
    "plot_metric(\"quality_loss_gt\", \"Quality Loss vs GT (%)\", \"../../plots/quality_loss_gt.pdf\")\n",
    "plot_metric(\"quality_loss_xl\", \"Quality Loss vs XL (%)\", \"../../plots/quality_loss_xl.pdf\")\n",
    "plot_metric(\"compute_savings_gt\", \"Compute Savings vs GT (%)\", \"../../plots/compute_savings_gt.pdf\")\n",
    "plot_metric(\"compute_savings_xl\", \"Compute Savings vs XL (%)\", \"../../plots/compute_savings_xl.pdf\")\n",
    "plot_metric(\"routing_accuracy\", \"Routing Accuracy (%)\", \"../../plots/routing_accuracy.pdf\")\n",
    "\n",
    "print(\"Plots saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ddc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
