{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb082fc",
   "metadata": {},
   "source": [
    "# Historical Runtime-Informed Routing Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e317c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628fda20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Historical Model Trained in 22.9189 seconds.\n",
      "Saved as: approach_3_pretrained_models/historical_model_p80.pkl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configuration\n",
    "exclude_percentile = 80  # Removes the top X% most complex queries\n",
    "train_ratio = 0.90       # 90% Training, 10% Testing\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_json('../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt0_tt3_rt3.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Filter out the top X% most complex queries\n",
    "complexity_threshold = df['query_complexity_score'].quantile(exclude_percentile / 100)\n",
    "filtered_df = df[df['query_complexity_score'] <= complexity_threshold].copy()\n",
    "num_removed = len(df) - len(filtered_df)\n",
    "\n",
    "print(f\"Filtered out the top {100 - exclude_percentile}% most complex queries.\")\n",
    "print(f\"Total queries removed: {num_removed}. Remaining for training/testing: {len(filtered_df)}.\")\n",
    "\n",
    "# Compute dynamic train-test split\n",
    "num_train = round(len(filtered_df) * train_ratio)  # First 90% for training\n",
    "num_test = len(filtered_df) - num_train            # Remaining 10% for testing\n",
    "\n",
    "train_df = filtered_df.iloc[:num_train].copy()     # Train set (first 90%)\n",
    "\n",
    "print(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\")\n",
    "\n",
    "# Keep only relevant columns for training\n",
    "train_df = train_df[['id', 'query', 'rt_t5_base_avg', 'rt_t5_large_avg', 'rt_t5_xl_avg',\n",
    "                 'r_t5_base_length_avg', 'r_t5_large_length_avg', 'r_t5_xl_length_avg',\n",
    "                 'r_t5_base_trash_count', 'r_t5_large_trash_count', 'r_t5_xl_trash_count',\n",
    "                 'r_t5_base_query_repetition_count', 'r_t5_large_query_repetition_count', 'r_t5_xl_query_repetition_count',\n",
    "                 'discrepancy_base_vs_large', 'discrepancy_large_vs_xl', 'discrepancy_base_vs_xl', 'evaluation_model_label']]\n",
    "\n",
    "# Extract X and Y for training\n",
    "X_train_historical = train_df[['query', 'rt_t5_base_avg', 'rt_t5_large_avg', 'rt_t5_xl_avg',\n",
    "                 'r_t5_base_length_avg', 'r_t5_large_length_avg', 'r_t5_xl_length_avg',\n",
    "                 'r_t5_base_trash_count', 'r_t5_large_trash_count', 'r_t5_xl_trash_count',\n",
    "                 'r_t5_base_query_repetition_count', 'r_t5_large_query_repetition_count', 'r_t5_xl_query_repetition_count',\n",
    "                 'discrepancy_base_vs_large', 'discrepancy_large_vs_xl', 'discrepancy_base_vs_xl']]\n",
    "y_train_historical = train_df['evaluation_model_label']\n",
    "\n",
    "# Train model\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000), 'query'),\n",
    "        ('scaler', StandardScaler(), [\n",
    "            'rt_t5_base_avg', 'rt_t5_large_avg', 'rt_t5_xl_avg',\n",
    "             'r_t5_base_length_avg', 'r_t5_large_length_avg', 'r_t5_xl_length_avg',\n",
    "             'r_t5_base_trash_count', 'r_t5_large_trash_count', 'r_t5_xl_trash_count',\n",
    "             'r_t5_base_query_repetition_count', 'r_t5_large_query_repetition_count', 'r_t5_xl_query_repetition_count',\n",
    "             'discrepancy_base_vs_large', 'discrepancy_large_vs_xl', 'discrepancy_base_vs_xl'\n",
    "        ])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline with the column transformer and Random Forest Classifier\n",
    "pipeline_historical = Pipeline([\n",
    "    ('transformer', column_transformer),\n",
    "    ('clf', RandomForestClassifier(n_estimators=200, max_depth=None, n_jobs=-1, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "pipeline_historical.fit(X_train_historical, y_train_historical)\n",
    "end_time = time.time()\n",
    "historical_train_time = end_time - start_time\n",
    "\n",
    "# Save category model with percentile in filename\n",
    "historical_model_filename = f\"trained_models/historical_model_p{exclude_percentile}.pkl\"\n",
    "joblib.dump(pipeline_historical, historical_model_filename)\n",
    "print(f\"Historical Model Trained in {historical_train_time:.4f} seconds.\")\n",
    "print(f\"Saved as: {historical_model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e216e6c",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cc0071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "Loaded 4000 queries for testing.\n",
      "Model Prediction Completed in 0.3181 seconds.\n",
      "Avg Time per Query (Inference Only): 0.000080 seconds\n",
      "\n",
      "MODEL EVALUATION\n",
      "Model Label Prediction Accuracy: 98.85%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      0.97      0.98       552\n",
      "           2       0.99      0.97      0.98       754\n",
      "           3       0.99      1.00      0.99      2694\n",
      "\n",
      "    accuracy                           0.99      4000\n",
      "   macro avg       0.99      0.98      0.98      4000\n",
      "weighted avg       0.99      0.99      0.99      4000\n",
      "\n",
      "Dataset with predicted model labels saved to: predicted_model_label.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import joblib\n",
    "from textstat import textstat\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "exclude_percentile = 80  # Exclude top X% most complex queries\n",
    "train_ratio = 0.90       # 90% Training, 10% Testing\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_json('../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt0_tt3_rt3.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Determine complexity threshold\n",
    "complexity_threshold = df['query_complexity_score'].quantile(exclude_percentile / 100)\n",
    "\n",
    "# Filter out the top X% most complex queries\n",
    "filtered_df = df[df['query_complexity_score'] <= complexity_threshold].copy()\n",
    "num_removed = len(df) - len(filtered_df)\n",
    "\n",
    "print(f\"Filtered out the top {100 - exclude_percentile}% most complex queries.\")\n",
    "print(f\"Total queries removed: {num_removed}. Remaining for training/testing: {len(filtered_df)}.\")\n",
    "\n",
    "# Compute train-test split\n",
    "num_train = round(len(filtered_df) * train_ratio)\n",
    "num_test = len(filtered_df) - num_train\n",
    "\n",
    "train_df = filtered_df.iloc[:num_train].copy()\n",
    "test_df = filtered_df.iloc[num_train:].copy()\n",
    "\n",
    "print(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\")\n",
    "\n",
    "# Load the correct model\n",
    "pipeline_historical = joblib.load(\"trained_models/historical_model_p80.pkl\")\n",
    "\n",
    "# Prepare testing data\n",
    "test_df = test_df[['id', 'query', 'rt_t5_base_avg', 'rt_t5_large_avg', 'rt_t5_xl_avg',\n",
    "                 'r_t5_base_length_avg', 'r_t5_large_length_avg', 'r_t5_xl_length_avg',\n",
    "                 'r_t5_base_trash_count', 'r_t5_large_trash_count', 'r_t5_xl_trash_count',\n",
    "                 'r_t5_base_query_repetition_count', 'r_t5_large_query_repetition_count', 'r_t5_xl_query_repetition_count',\n",
    "                 'discrepancy_base_vs_large', 'discrepancy_large_vs_xl', 'discrepancy_base_vs_xl']]\n",
    "print(f\"Loaded {len(test_df)} queries for testing.\")\n",
    "\n",
    "# Label prediction\n",
    "test_X_historical= test_df.drop(columns=['id'], errors='ignore')\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = pipeline_historical.predict(test_X_historical)\n",
    "end_time = time.time()\n",
    "historical_test_time = end_time - start_time\n",
    "avg_time_per_query = historical_test_time / len(test_df)\n",
    "\n",
    "print(f\"Model Prediction Completed in {historical_test_time:.4f} seconds.\")\n",
    "print(f\"Avg Time per Query (Inference Only): {avg_time_per_query:.6f} seconds\")\n",
    "\n",
    "# Append predicted model labels to test_df\n",
    "test_df['predicted_model_label'] = y_pred\n",
    "\n",
    "# Restore actual labels for evaluation\n",
    "test_df['evaluation_model_label'] = filtered_df.iloc[num_train:]['evaluation_model_label'].values\n",
    "\n",
    "# Model evaluation\n",
    "historical_accuracy = accuracy_score(test_df['evaluation_model_label'], test_df['predicted_model_label'])\n",
    "historical_report = classification_report(test_df['evaluation_model_label'], test_df['predicted_model_label'])\n",
    "\n",
    "print(\"\\nMODEL EVALUATION\")\n",
    "print(f\"Model Label Prediction Accuracy: {historical_accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", historical_report)\n",
    "\n",
    "# Define output file path\n",
    "output_file_path = 'predicted_model_label.jsonl'\n",
    "\n",
    "# Write DataFrame to a JSONL file including probabilities\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for entry in test_df[['id', 'predicted_model_label']].to_dict(orient='records'):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write('\\n')\n",
    "        \n",
    "print(f\"Dataset with predicted model labels saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86532cf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638e9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out the top 20% most complex queries.\n",
      "Total queries removed: 9996. Remaining for training/testing: 40004.\n",
      "Final split: 36004 queries for training, 4000 queries for testing.\n",
      "\n",
      "--- Routing Heuristic Evaluation ---\n",
      "Quality Loss compared to GT: -0.03%\n",
      "Quality Loss compared to XL: 2.98%\n",
      "Compute Savings compared to GT: -0.95%\n",
      "Compute Savings compared to XL: 25.90%\n",
      "Routing Accuracy: 98.85%\n",
      "Total Compute Units Used: 35566.20 CUs\n",
      "Number of queries routed to Base: 537\n",
      "Number of queries routed to Large: 735\n",
      "Number of queries routed to XL: 2728\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "exclude_percentile = 80  # Exclude top X% most complex queries\n",
    "train_ratio = 0.90       # 90% Training, 10% Testing\n",
    "\n",
    "# Define compute unit consumption for each model\n",
    "COMPUTE_UNITS = {1: 1.0, 2: 3.12, 3: 12.0}\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_json('../../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt0_tt3_rt3.jsonl',orient='records', lines=True)\n",
    "\n",
    "# Determine complexity threshold\n",
    "complexity_threshold = df['query_complexity_score'].quantile(exclude_percentile / 100)\n",
    "\n",
    "# Filter out the top X% most complex queries\n",
    "filtered_df = df[df['query_complexity_score'] <= complexity_threshold].copy()\n",
    "num_removed = len(df) - len(filtered_df)\n",
    "\n",
    "print(f\"Filtered out the top {100 - exclude_percentile}% most complex queries.\")\n",
    "print(f\"Total queries removed: {num_removed}. Remaining for training/testing: {len(filtered_df)}.\")\n",
    "\n",
    "# Compute dynamic train-test split\n",
    "num_train = round(len(filtered_df) * train_ratio)  # First 90% for training\n",
    "num_test = len(filtered_df) - num_train            # Remaining 10% for testing\n",
    "\n",
    "train_df = filtered_df.iloc[:num_train].copy()     # Train set (first 90%)\n",
    "test_df = filtered_df.iloc[num_train:].copy()      # Test set (remaining 10%)\n",
    "\n",
    "print(f\"Final split: {num_train} queries for training, {num_test} queries for testing.\")\n",
    "\n",
    "# Load predicted labels from saved file\n",
    "predicted_labels_df = pd.read_json('predicted_model_label.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Merge complexity scores with test data\n",
    "test_df = test_df.merge(predicted_labels_df, on='id', how='left')\n",
    "\n",
    "# Check if merge worked\n",
    "if 'predicted_model_label' not in test_df.columns:\n",
    "    raise KeyError(\"Error: 'predicted_model_label' is missing after merging!\")\n",
    "    \n",
    "def calculate_quality_discrepancies(df):\n",
    "    \"\"\"\n",
    "    Computes the quality loss when routing based on predicted labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with routing results.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (quality_loss_to_gt, quality_loss_to_xl)\n",
    "    \"\"\"\n",
    "    total_score_gt = df.apply(lambda row: row[f\"avg_normalized_score_t5_{['base', 'large', 'xl'][row['evaluation_model_label'] - 1]}\"], axis=1).sum()\n",
    "    total_score_router = df.apply(lambda row: row[f\"avg_normalized_score_t5_{['base', 'large', 'xl'][row['predicted_model_label'] - 1]}\"], axis=1).sum()\n",
    "    total_score_xl = df['avg_normalized_score_t5_xl'].sum()\n",
    "\n",
    "    num_queries = len(df)\n",
    "\n",
    "    avg_score_gt = total_score_gt / num_queries\n",
    "    avg_score_router = total_score_router / num_queries\n",
    "    avg_score_xl = total_score_xl / num_queries\n",
    "\n",
    "    quality_loss_to_gt = ((avg_score_router - avg_score_gt) / avg_score_gt) * 100\n",
    "    quality_loss_to_xl = ((avg_score_router - avg_score_xl) / avg_score_xl) * 100\n",
    "\n",
    "    return quality_loss_to_gt, quality_loss_to_xl\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(test_df['evaluation_model_label'], test_df['predicted_model_label']) * 100\n",
    "quality_loss_gt, quality_loss_xl = calculate_quality_discrepancies(test_df)\n",
    "\n",
    "# Count queries routed to each model\n",
    "num_routed_base = (test_df['predicted_model_label'] == 1).sum()\n",
    "num_routed_large = (test_df['predicted_model_label'] == 2).sum()\n",
    "num_routed_xl = (test_df['predicted_model_label'] == 3).sum()\n",
    "\n",
    "# Define compute usage and savings\n",
    "compute_units_gt = sum(test_df['evaluation_model_label'].map(COMPUTE_UNITS))\n",
    "compute_units_router = sum(test_df['predicted_model_label'].map(COMPUTE_UNITS))\n",
    "compute_units_xl = len(test_df) * COMPUTE_UNITS[3]\n",
    "\n",
    "compute_savings_gt = ((compute_units_gt - compute_units_router) / compute_units_gt) * 100\n",
    "compute_savings_xl = ((compute_units_xl - compute_units_router) / compute_units_xl) * 100\n",
    "\n",
    "# Print final evaluation metrics\n",
    "print(\"\\n--- Routing Heuristic Evaluation ---\")\n",
    "print(f\"Quality Loss compared to GT: {quality_loss_gt:.2f}%\")\n",
    "print(f\"Quality Loss compared to XL: {quality_loss_xl:.2f}%\")\n",
    "print(f\"Compute Savings compared to GT: {compute_savings_gt:.2f}%\")\n",
    "print(f\"Compute Savings compared to XL: {compute_savings_xl:.2f}%\")\n",
    "print(f\"Routing Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Total Compute Units Used: {compute_units_router:.2f} CUs\")\n",
    "print(f\"Number of queries routed to Base: {num_routed_base}\")\n",
    "print(f\"Number of queries routed to Large: {num_routed_large}\")\n",
    "print(f\"Number of queries routed to XL: {num_routed_xl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b4647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
