{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Complexity Estimation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Empty Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 50k rows created and saved in ../datasets/generated/step_1/empty_feature_dataset_50k.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_feature_dataset(num_rows):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame with a predefined feature template for query evaluation.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): The number of rows to generate in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing initialized feature columns for each row.\n",
    "    \"\"\"\n",
    "    feature_template = {\n",
    "        \"id\": \"\",\n",
    "        \"instruction\": \"\",\n",
    "        \"input\": \"\",\n",
    "        \"query\": \"\",\n",
    "        \"category\": \"\",\n",
    "        \"output\": \"\",\n",
    "        \"query_chars_count\": 0,\n",
    "        \"query_words_count\": 0,\n",
    "        \"query_unique_word_count\": 0,\n",
    "        \"query_readability_score\": 0.0,\n",
    "        \"query_special_tokens_count\": 0,\n",
    "        \"query_keywords_count\": 0,\n",
    "        \"query_contains_url\": 0,\n",
    "        \"query_complexity_score\": 0.0,\n",
    "        \"rt_t5_base_avg\": 0.0,\n",
    "        \"rt_t5_large_avg\": 0.0,\n",
    "        \"rt_t5_xl_avg\": 0.0,\n",
    "        \"r_t5_base_length_avg\": 0.0,\n",
    "        \"r_t5_large_length_avg\": 0.0,\n",
    "        \"r_t5_xl_length_avg\": 0.0,\n",
    "        \"r_t5_base_redundancy_avg\": 0.0,\n",
    "        \"r_t5_large_redundancy_avg\": 0.0,\n",
    "        \"r_t5_xl_redundancy_avg\": 0.0,\n",
    "        \"r_t5_base_trash_count\": 0,\n",
    "        \"r_t5_large_trash_count\": 0,\n",
    "        \"r_t5_xl_trash_count\": 0,\n",
    "        \"r_t5_base_query_repetition_count\": 0,\n",
    "        \"r_t5_large_query_repetition_count\": 0,\n",
    "        \"r_t5_xl_query_repetition_count\": 0,\n",
    "        \"scores_t5_base_avg\": {\n",
    "            \"bart\": 0.0,\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0,\n",
    "            \"bleu\": 0.0,\n",
    "            \"bert\": 0.0,\n",
    "            \"bleurt\": 0.0,\n",
    "            \"logprobs\": 0.0\n",
    "        },\n",
    "        \"scores_t5_large_avg\": {\n",
    "            \"bart\": 0.0,\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0,\n",
    "            \"bleu\": 0.0,\n",
    "            \"bert\": 0.0,\n",
    "            \"bleurt\": 0.0,\n",
    "            \"logprobs\": 0.0\n",
    "        },\n",
    "        \"scores_t5_xl_avg\": {\n",
    "            \"bart\": 0.0,\n",
    "            \"rouge1\": 0.0,\n",
    "            \"rouge2\": 0.0,\n",
    "            \"rougeL\": 0.0,\n",
    "            \"rougeLsum\": 0.0,\n",
    "            \"bleu\": 0.0,\n",
    "            \"bert\": 0.0,\n",
    "            \"bleurt\": 0.0,\n",
    "            \"logprobs\": 0.0\n",
    "        },\n",
    "        \"avg_normalized_score_t5_base\": 0.0,\n",
    "        \"avg_normalized_score_t5_large\": 0.0,\n",
    "        \"avg_normalized_score_t5_xl\": 0.0,\n",
    "        \"discrepancy_base_vs_large\": 0.0,\n",
    "        \"discrepancy_large_vs_xl\": 0.0,\n",
    "        \"discrepancy_base_vs_xl\": 0.0,\n",
    "        \"evaluation_model_label\": 0\n",
    "    }\n",
    "    return pd.DataFrame([feature_template] * num_rows)\n",
    "\n",
    "# Create the dataset with 50k rows\n",
    "num_rows = 50000\n",
    "feature_df = create_feature_dataset(num_rows)\n",
    "\n",
    "# Define output file path\n",
    "output_file_path = '../datasets/generated/step_1/empty_feature_dataset_50k.jsonl'\n",
    "\n",
    "# Write DataFrame to a JSONL file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for entry in feature_df.to_dict(orient='records'):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write('\\n')\n",
    "\n",
    "print(f\"Dataset with 50k rows created and saved in {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Empty Response Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 50k rows created and saved in ../datasets/generated/step_2/empty_response_dataset_50k.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_response_dataset(num_rows):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame for storing responses and evaluation scores for different models.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int): The number of rows to generate in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing response fields and evaluation scores for each model.\n",
    "    \"\"\"\n",
    "    response_template = {\"id\": \"\"}\n",
    "    for model in [\"t5_base\", \"t5_large\", \"t5_xl\"]:\n",
    "        # Add 10 responses and scores per model \n",
    "        for i in range(1, 11):\n",
    "            response_template[f\"r_{model}_{i}\"] = \"\"\n",
    "            response_template[f\"scores_{model}_{i}\"] = {\n",
    "                \"bart\": 0.0,\n",
    "                \"rouge1\": 0.0,\n",
    "                \"rouge2\": 0.0,\n",
    "                \"rougeL\": 0.0,\n",
    "                \"rougeLsum\": 0.0,\n",
    "                \"bleu\": 0.0,\n",
    "                \"bert\": 0.0,\n",
    "                \"bleurt\": 0.0,\n",
    "                \"logprobs\": 0.0\n",
    "            }\n",
    "        # Add average response time feature\n",
    "        response_template[f\"rt_{model}_avg\"] = 0.0\n",
    "    return pd.DataFrame([response_template] * num_rows)\n",
    "\n",
    "# Create the dataset with 50k rows\n",
    "num_rows = 50000\n",
    "response_df = create_response_dataset(num_rows)\n",
    "\n",
    "# Define output file path\n",
    "output_file_path = '../datasets/generated/step_2/empty_response_dataset_50k.jsonl'\n",
    "\n",
    "# Write DataFrame to a JSONL file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for entry in response_df.to_dict(orient='records'):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write('\\n')\n",
    "\n",
    "print(f\"Dataset with 50k rows created and saved in {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Reduce MixInstruct Dataset to 50,000 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset saved to ../datasets/generated/step_3/train_data_reduced_50k.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Path to the original dataset\n",
    "input_file = \"../datasets/mix-instruct/train_data_prepared.jsonl\"\n",
    "output_file = \"../datasets/generated/step_3/train_data_reduced_50k.jsonl\"\n",
    "\n",
    "# Load the original dataset\n",
    "df = pd.read_json(input_file, lines=True)\n",
    "\n",
    "# Select only the required columns and reduce to 50k rows\n",
    "df = df[[\"id\", \"instruction\", \"input\", \"output\"]].head(50000)\n",
    "\n",
    "# Save to JSONL format\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Reduced dataset saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Categorize Queries\n",
    "#### ATTENTION: Output file might need manual formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "client = openai.OpenAI(api_key='PLACE_KEY_HERE')\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"../datasets/generated/step_3/train_data_reduced_50k.jsonl\"\n",
    "output_file_path = \"../datasets/generated/step_4/train_data_categorized_50k_unformatted.json\"\n",
    "\n",
    "# Request template text\n",
    "request_text_template = \"\"\"Please categorize following queries (instruction+input) into one of following 14 categories: \n",
    "1. Factual Question Answering (QA), \n",
    "2. Open-ended/Opinion-based Queries \n",
    "3. Creative Text Generation \n",
    "4. Instruction Following\n",
    "5. Mathematics/Problem Solving \n",
    "6. Recommendation Systems \n",
    "7. Coding/Programming \n",
    "8. Summarization/ParaphrasingTasks \n",
    "9. Translation Tasks \n",
    "10. Reasoning/Logic Questions \n",
    "11. Dialogue/Conversational Queries \n",
    "12. Comparative/Analytical Queries \n",
    "13. Miscellaneous/Other \n",
    "14. General Advice/Personal Tasks\n",
    "\n",
    "by adding a new column \"category\" to each query. Please only return the query ID and its category number to me in a format like: {\"id\":\"itwgpt4\\/33289\",\"category\":7}, ... Thanks!\"\"\"\n",
    "\n",
    "def send_gpt_request(queries):\n",
    "    \"\"\"\n",
    "    Sends a batch of queries to the GPT-4 model for categorization.\n",
    "\n",
    "    Args:\n",
    "        queries (list): List of queries to be categorized.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from GPT-4 containing categorized queries, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    prompt = request_text_template + \"\\n\" + \"\\n\".join(queries)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=4095,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_queries(start_index, num_queries):\n",
    "    \"\"\"\n",
    "    Loads a subset of queries from the input JSONL file.\n",
    "\n",
    "    Args:\n",
    "        start_index (int): The index in the dataset from which to start loading.\n",
    "        num_queries (int): The number of queries to load.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of query strings.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        queries = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= start_index and i < start_index + num_queries:\n",
    "                queries.append(line.strip())\n",
    "            if i >= start_index + num_queries:\n",
    "                break\n",
    "    return queries\n",
    "\n",
    "def append_results_to_file(results):\n",
    "    \"\"\"\n",
    "    Appends the categorized queries to the output JSON file.\n",
    "\n",
    "    Args:\n",
    "        results (str): The formatted JSON string containing categorized queries.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'a') as f:\n",
    "        f.write(results + \"\\n\")\n",
    "\n",
    "def countdown(seconds):\n",
    "    \"\"\"\n",
    "    Implements a countdown timer before sending the next API request.\n",
    "\n",
    "    Args:\n",
    "        seconds (int): The number of seconds to wait before the next request.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for i in range(seconds, 0, -1):\n",
    "        print(f\"Next request in {i} seconds...\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "def process_queries(start_index, num_queries_per_request, total_requests, wait_time_between_requests):\n",
    "    \"\"\"\n",
    "    Processes queries in batches, sends them to GPT-4 for categorization, and saves the results.\n",
    "\n",
    "    Args:\n",
    "        start_index (int): The starting index in the dataset.\n",
    "        num_queries_per_request (int): The number of queries to send in each request.\n",
    "        total_requests (int): The total number of requests to process.\n",
    "        wait_time_between_requests (int): The wait time (in seconds) between API requests.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for request_number in range(total_requests):\n",
    "        print(f\"Processing request {request_number + 1}/{total_requests}...\")\n",
    "\n",
    "        # Load the queries for the current batch\n",
    "        queries = load_queries(start_index + request_number * num_queries_per_request, num_queries_per_request)\n",
    "\n",
    "        if not queries:\n",
    "            print(\"No more queries to process.\")\n",
    "            break\n",
    "\n",
    "        # Send the request to GPT-4\n",
    "        result = send_gpt_request(queries)\n",
    "        if result:\n",
    "            # Append the result to the output file\n",
    "            append_results_to_file(result)\n",
    "\n",
    "        # Countdown before the next request\n",
    "        if request_number < total_requests - 1:\n",
    "            countdown(wait_time_between_requests)\n",
    "\n",
    "# Parameters\n",
    "start_index = 0  # Set the starting index in the dataset\n",
    "num_queries_per_request = 230  # Number of queries sent in each API request. Higher values risk exceeding token limits.\n",
    "total_requests = 218  # Total number of API requests needed to process all 50,000 queries (50000 / 230, rounded up).\n",
    "wait_time_between_requests = 10  # Time in seconds between requests to avoid rate limits.\n",
    "\n",
    "# Start processing the queries\n",
    "process_queries(start_index, num_queries_per_request, total_requests, wait_time_between_requests)\n",
    "\n",
    "print(f\"Process completed. Results printed to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fill empty Feature Dataset with MixInstruct Queries and Calculate Query Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dataset created and saved to ../datasets/generated/step_5/train_data_with_query_features.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from textstat import textstat\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a record from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def construct_query(instruction, input_text):\n",
    "    \"\"\"\n",
    "    Constructs a query by combining the instruction and input text.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The instruction part of the query.\n",
    "        input_text (str): The input text associated with the instruction.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted query string.\n",
    "    \"\"\"\n",
    "    if instruction and input_text:\n",
    "        return f\"Instruction: {instruction} Input: {input_text}\"\n",
    "    elif instruction:\n",
    "        return f\"Instruction: {instruction}\"\n",
    "    else:\n",
    "        return f\"Input: {input_text}\"\n",
    "\n",
    "# Load the empty feature dataset\n",
    "feature_file = \"../datasets/generated/step_1/empty_feature_dataset_50k.jsonl\"\n",
    "feature_df = pd.DataFrame(read_jsonl(feature_file))\n",
    "\n",
    "# Load the mix-instruct dataset\n",
    "mix_instruct_file = \"../datasets/generated/step_3/train_data_reduced_50k.jsonl\"\n",
    "mix_instruct_df = pd.DataFrame(read_jsonl(mix_instruct_file))\n",
    "\n",
    "# Copy values from the mix-instruct dataset into the feature dataset\n",
    "feature_df[\"id\"] = mix_instruct_df[\"id\"]\n",
    "feature_df[\"instruction\"] = mix_instruct_df[\"instruction\"]\n",
    "feature_df[\"input\"] = mix_instruct_df[\"input\"]\n",
    "feature_df[\"output\"] = mix_instruct_df[\"output\"]\n",
    "\n",
    "# Fill \"query\" feature\n",
    "feature_df[\"query\"] = feature_df.apply(\n",
    "    lambda row: construct_query(row[\"instruction\"], row[\"input\"]), axis=1\n",
    ")\n",
    "\n",
    "# Load categorized data to fill the \"category\" feature\n",
    "category_file = \"../Datasets/generated/step_4/train_data_categorized_50k.jsonl\"\n",
    "category_df = pd.DataFrame(read_jsonl(category_file))\n",
    "\n",
    "# Map categories using the predefined mapping\n",
    "category_mapping = {\n",
    "    1: \"Factual Question Answering\",\n",
    "    2: \"Open-ended/Opinion-based\",\n",
    "    3: \"Creative Text Generation\",\n",
    "    4: \"Instruction Following\",\n",
    "    5: \"Mathematics/Problem Solving\",\n",
    "    6: \"Recommendation Systems\",\n",
    "    7: \"Coding/Programming\",\n",
    "    8: \"Summarization/Paraphrasing\",\n",
    "    9: \"Translation\",\n",
    "    10: \"Reasoning/Logic\",\n",
    "    11: \"Dialogue/Conversational\",\n",
    "    12: \"Comparative/Analytical\",\n",
    "    13: \"Miscellaneous/Other\",\n",
    "    14: \"General Advice/Personal\"\n",
    "}\n",
    "\n",
    "# Map the categories\n",
    "feature_df[\"category\"] = category_df[\"category\"].map(category_mapping)\n",
    "\n",
    "# Define the list of special tokens\n",
    "special_tokens = [\":\", \";\", \"=\", \"+\", \"-\", \"_\", \"/\", \".\", \"'\", '\"', \"´\", \"`\", \",\", \"<\", \">\", \"[\", \"]\", \"{\", \"}\", \"(\", \")\", \"?\", \"!\", \"*\", \"&\", \"$\", \"#\", \"@\", \"%\", \"^\", \"~\", \"|\", \"\\\\\"]\n",
    "\n",
    "# Define a list of keywords relevant for query complexity\n",
    "keywords = [\n",
    "    \"analyze\", \"synthesize\", \"interpret\", \"evaluate\", \"justify\", \"compare\", \n",
    "    \"optimize\", \"hypothesize\", \"formulate\", \"simulate\", \"derive\", \"describe\",\n",
    "    \"validate\", \"correlate\", \"quantify\", \"investigate\", \"predict\", \"forecast\",\n",
    "    \"prove\", \"assess\", \"criticize\", \"argue\", \"solve\", \"reconstruct\", \"theorize\",\n",
    "    \"explore\", \"elaborate\", \"deduce\", \"refute\", \"conceptualize\", \"identify\", \"outline\",\n",
    "    \"rationalize\", \"articulate\", \"summarize\", \"innovate\", \"extrapolate\", \"explain\", \"clarify\"\n",
    "]\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes URLs and image links from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text without URLs.\n",
    "    \"\"\"\n",
    "    return re.sub(r'!\\[.*?\\]\\(.*?\\)|http[s]?://\\S+', '', text)\n",
    "\n",
    "# Define the feature calculations\n",
    "query_features = {\n",
    "    'query_chars_count': lambda row: len(row['query']),\n",
    "    'query_words_count': lambda row: len(row['query'].split()),\n",
    "    'query_unique_word_count': lambda row: len(set(row['query'].split())),\n",
    "    'query_readability_score': lambda row: textstat.flesch_kincaid_grade(remove_urls(row['query'])),\n",
    "    'query_special_tokens_count': lambda row: sum(1 for char in row['query'] if char in special_tokens),\n",
    "    'query_keywords_count': lambda row: (\n",
    "        sum(len(re.findall(r'\\b{}\\b'.format(re.escape(keyword)), row['instruction'].lower())) for keyword in keywords)\n",
    "        if row['instruction'] \n",
    "        else sum(len(re.findall(r'\\b{}\\b'.format(re.escape(keyword)), row['input'].lower())) for keyword in keywords)\n",
    "    ),\n",
    "    'query_contains_url': lambda row: int(bool(re.search(r'http[s]?://', row['query'])))\n",
    "}\n",
    "\n",
    "# Apply the feature calculations to the dataset\n",
    "for feature_name, feature_func in query_features.items():\n",
    "    feature_df[feature_name] = feature_df.apply(feature_func, axis=1)\n",
    "\n",
    "# Save the updated dataset\n",
    "output_file_path = \"../datasets/generated/step_5/train_data_with_query_features.jsonl\"\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for entry in feature_df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "print(f\"Feature dataset created and saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate Complexity Score for each Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min complexity score: 0.5207\n",
      "Max complexity score: 5.2306\n",
      "Complexity scores calculated and saved to <_io.TextIOWrapper name='../datasets/generated/step_6/train_data_with_query_features_complexity_scores.jsonl' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a record from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"../datasets/generated/step_5/train_data_with_query_features.jsonl\"\n",
    "output_file_path = \"../datasets/generated/step_6/train_data_with_query_features_complexity_scores.jsonl\"\n",
    "\n",
    "# Load dataset\n",
    "feature_df = pd.DataFrame(read_jsonl(input_file_path))\n",
    "\n",
    "# Normalized category weights based on subjective assessment of complexity\n",
    "category_weights = {\n",
    "    \"Mathematics/Problem Solving\": 1.0,\n",
    "    \"Coding/Programming\": 0.9,\n",
    "    \"Reasoning/Logic\": 0.85,\n",
    "    \"Summarization/Paraphrasing\": 0.75,\n",
    "    \"Comparative/Analytical\": 0.7,\n",
    "    \"Creative Text Generation\": 0.6,\n",
    "    \"Open-ended/Opinion-based\": 0.5,\n",
    "    \"Recommendation Systems\": 0.45,\n",
    "    \"General Advice/Personal\": 0.4,\n",
    "    \"Dialogue/Conversational\": 0.35,\n",
    "    \"Factual Question Answering\": 0.3,\n",
    "    \"Instruction Following\": 0.25,\n",
    "    \"Translation\": 0.2,\n",
    "    \"Miscellaneous/Other\": 0.15,\n",
    "}\n",
    "\n",
    "# Calculate min and max values for each feature\n",
    "feature_min_max = {\n",
    "    \"query_chars_count\": (feature_df['query_chars_count'].min(), feature_df['query_chars_count'].max()),\n",
    "    \"query_words_count\": (feature_df['query_words_count'].min(), feature_df['query_words_count'].max()),\n",
    "    \"query_unique_word_count\": (feature_df['query_unique_word_count'].min(), feature_df['query_unique_word_count'].max()),\n",
    "    \"query_readability_score\": (feature_df['query_readability_score'].min(), 20),  # Cap readability at 20\n",
    "    \"query_special_tokens_count\": (feature_df['query_special_tokens_count'].min(), feature_df['query_special_tokens_count'].max()),\n",
    "    \"query_keywords_count\": (feature_df['query_keywords_count'].min(), feature_df['query_keywords_count'].max()),\n",
    "}\n",
    "\n",
    "# Define feature weights for complexity calculation\n",
    "weights = {\n",
    "    \"category\": 2.5,\n",
    "    \"length\": 1.0,\n",
    "    \"unique\": 0.4,\n",
    "    \"readability\": 1.2,\n",
    "    \"special_tokens\": 0.9,\n",
    "    \"keywords\": 1.5,\n",
    "    \"url_presence\": 1.0,\n",
    "}\n",
    "\n",
    "def min_max_scaling(value, min_value, max_value):\n",
    "    \"\"\"\n",
    "    Applies Min-Max scaling to normalize a feature value between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "        value (float): The feature value to be scaled.\n",
    "        min_value (float): The minimum value of the feature.\n",
    "        max_value (float): The maximum value of the feature.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized value between 0 and 1.\n",
    "    \"\"\"\n",
    "    if max_value - min_value == 0:\n",
    "        return 0  # Handle division by zero for constant features\n",
    "    return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "def calculate_complexity(row):\n",
    "    \"\"\"\n",
    "    Computes the query complexity score based on multiple features.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): A row from the dataset containing query-related features.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated complexity score for the query.\n",
    "    \"\"\"\n",
    "    category_weight = category_weights.get(row['category'], 0.03)  # Default weight if category unknown\n",
    "\n",
    "    # Min-max scaled query length features\n",
    "    length_score = (\n",
    "        min_max_scaling(row['query_chars_count'], *feature_min_max['query_chars_count']) +\n",
    "        min_max_scaling(row['query_words_count'], *feature_min_max['query_words_count'])\n",
    "    ) / 2\n",
    "\n",
    "    # Unique words\n",
    "    unique_score = min_max_scaling(row['query_unique_word_count'], *feature_min_max['query_unique_word_count'])\n",
    "\n",
    "    # Readability score\n",
    "    readability_score = min_max_scaling(\n",
    "        min(row['query_readability_score'], feature_min_max['query_readability_score'][1]),\n",
    "        *feature_min_max['query_readability_score']\n",
    "    )\n",
    "\n",
    "    # Special tokens count\n",
    "    special_tokens_score = min_max_scaling(row['query_special_tokens_count'], *feature_min_max['query_special_tokens_count'])\n",
    "\n",
    "    # Keywords count\n",
    "    keywords_score = min_max_scaling(row['query_keywords_count'], *feature_min_max['query_keywords_count'])\n",
    "\n",
    "    # URL presence score\n",
    "    url_presence_score = row['query_contains_url']\n",
    "\n",
    "    # Combine scores\n",
    "    complexity_score = (\n",
    "        weights['category'] * category_weight +\n",
    "        weights['length'] * length_score +\n",
    "        weights['unique'] * unique_score +\n",
    "        weights['readability'] * readability_score +\n",
    "        weights['special_tokens'] * special_tokens_score +\n",
    "        weights['keywords'] * keywords_score +\n",
    "        weights['url_presence'] * url_presence_score\n",
    "    )\n",
    "\n",
    "    return round(complexity_score, 4)\n",
    "\n",
    "# Apply complexity calculation to each row\n",
    "feature_df['query_complexity_score'] = feature_df.apply(calculate_complexity, axis=1)\n",
    "\n",
    "# Calculate and print the min and max range of the complexity score\n",
    "min_complexity_score = feature_df['query_complexity_score'].min()\n",
    "max_complexity_score = feature_df['query_complexity_score'].max()\n",
    "\n",
    "print(f\"Min complexity score: {min_complexity_score}\")\n",
    "print(f\"Max complexity score: {max_complexity_score}\")\n",
    "\n",
    "# Save the dataset with complexity scores\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for entry in feature_df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "print(f\"Complexity scores calculated and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Generate all Responses from all Flan-T5-Models with Evaluation Scripts from \"./scripts/response_generation/\"\n",
    "#### ATTENTION: Run each script in predefined virtual environment on a GPU with enough RAM (40GB for XL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Calculate all Metric Scores with Scripts from \"./scripts/metric_scoring\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Merge Responses and Metric Scores into Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing base dataset...\n",
      "Train data updated and saved to ../datasets/generated/step_9/train_data_with_50kx10_t5_base_answers_with_scores.jsonl.\n",
      "Processing large dataset...\n",
      "Train data updated and saved to ../datasets/generated/step_9/train_data_with_50kx10_t5_large_answers_with_scores.jsonl.\n",
      "Processing xl dataset...\n",
      "Train data updated and saved to ../datasets/generated/step_9/train_data_with_50kx10_t5_xl_answers_with_scores.jsonl.\n",
      "All datasets successfully updated with scores.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the paths for train files and their corresponding score files\n",
    "datasets = {\n",
    "    \"base\": {\n",
    "        \"train_file\": \"../datasets/generated/step_7/train_data_with_50k_times_10_t5_base_answers_with_avgtime_max5000.jsonl\",\n",
    "        \"score_files\": {\n",
    "            \"bart\": \"../datasets/generated/step_8/bart_scores_t5_base.jsonl\",\n",
    "            \"bert\": \"../datasets/generated/step_8/bert_scores_t5_base.jsonl\",\n",
    "            \"bleu\": \"../datasets/generated/step_8/bleu_scores_t5_base.jsonl\",\n",
    "            \"bleurt\": \"../datasets/generated/step_8/bleurt_scores_t5_base.jsonl\",\n",
    "            \"logprobs\": \"../datasets/generated/step_8/logprobs_scores_t5_base.jsonl\",\n",
    "            \"rouge\": \"../datasets/generated/step_8/rouge_scores_t5_base.jsonl\"\n",
    "        },\n",
    "        \"output_file\": \"../datasets/generated/step_9/train_data_with_50kx10_t5_base_answers_with_scores.jsonl\",\n",
    "        \"model_name\": \"base\"\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"train_file\": \"../datasets/generated/step_7/train_data_with_50k_times_10_t5_large_answers_with_avgtime_max5000.jsonl\",\n",
    "        \"score_files\": {\n",
    "            \"bart\": \"../datasets/generated/step_8/bart_scores_t5_large.jsonl\",\n",
    "            \"bert\": \"../datasets/generated/step_8/bert_scores_t5_large.jsonl\",\n",
    "            \"bleu\": \"../datasets/generated/step_8/bleu_scores_t5_large.jsonl\",\n",
    "            \"bleurt\": \"../datasets/generated/step_8/bleurt_scores_t5_large.jsonl\",\n",
    "            \"logprobs\": \"../datasets/generated/step_8/logprobs_scores_t5_large.jsonl\",\n",
    "            \"rouge\": \"../datasets/generated/step_8/rouge_scores_t5_large.jsonl\"\n",
    "        },\n",
    "        \"output_file\": \"../datasets/generated/step_9/train_data_with_50kx10_t5_large_answers_with_scores.jsonl\",\n",
    "        \"model_name\": \"large\"\n",
    "    },\n",
    "    \"xl\": {\n",
    "        \"train_file\": \"../datasets/generated/step_7/train_data_with_50k_times_10_t5_xl_answers_with_avgtime_max5000.jsonl\",\n",
    "        \"score_files\": {\n",
    "            \"bart\": \"../datasets/generated/step_8/bart_scores_t5_xl.jsonl\",\n",
    "            \"bert\": \"../datasets/generated/step_8/bert_scores_t5_xl.jsonl\",\n",
    "            \"bleu\": \"../datasets/generated/step_8/bleu_scores_t5_xl.jsonl\",\n",
    "            \"bleurt\": \"../datasets/generated/step_8/bleurt_scores_t5_xl.jsonl\",\n",
    "            \"logprobs\": \"../datasets/generated/step_8/logprobs_scores_t5_xl.jsonl\",\n",
    "            \"rouge\": \"../datasets/generated/step_8/rouge_scores_t5_xl.jsonl\"\n",
    "        },\n",
    "        \"output_file\": \"../datasets/generated/step_9/train_data_with_50kx10_t5_xl_answers_with_scores.jsonl\",\n",
    "        \"model_name\": \"xl\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define metrics for each score file\n",
    "score_metrics = {\n",
    "    \"bart\": [\"bart\"],\n",
    "    \"bert\": [\"bert\"],\n",
    "    \"bleu\": [\"bleu\"],\n",
    "    \"bleurt\": [\"bleurt\"],\n",
    "    \"logprobs\": [\"logprobs\"],\n",
    "    \"rouge\": [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "}\n",
    "\n",
    "def load_score_file(score_file, metrics, model_name):\n",
    "    \"\"\"\n",
    "    Loads a score file and organizes its content in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        score_file (str): Path to the JSONL file containing scores.\n",
    "        metrics (list): List of metric names to extract.\n",
    "        model_name (str): Model identifier (base, large, xl) used for key structuring.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with query IDs as keys and metric scores for each response (1 to 10).\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    with open(score_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            scores[data[\"id\"]] = {\n",
    "                f\"scores_t5_{model_name}_{i}\": {\n",
    "                    metric: data[f\"scores_t5_{model_name}_{i}\"][metric]\n",
    "                    for metric in metrics\n",
    "                }\n",
    "                for i in range(1, 11)\n",
    "            }\n",
    "    return scores\n",
    "\n",
    "def update_train_file(train_file, score_files, output_file, model_name):\n",
    "    \"\"\"\n",
    "    Updates a train file with scores from corresponding score files.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Path to the JSONL file containing training data.\n",
    "        score_files (dict): Dictionary mapping score types to their file paths.\n",
    "        output_file (str): Path to save the updated train dataset.\n",
    "        model_name (str): Model identifier (base, large, xl).\n",
    "\n",
    "    Returns:\n",
    "        None: The function writes the updated dataset to the output file.\n",
    "    \"\"\"\n",
    "    # Load all score files into dictionaries\n",
    "    score_data = {\n",
    "        metric: load_score_file(file, metrics, model_name)\n",
    "        for metric, (file, metrics) in zip(score_files.keys(), zip(score_files.values(), score_metrics.values()))\n",
    "    }\n",
    "\n",
    "    # Update train data\n",
    "    with open(train_file, \"r\") as train_f, open(output_file, \"w\") as out_f:\n",
    "        for train_line in train_f:\n",
    "            train_data = json.loads(train_line)\n",
    "            train_id = train_data[\"id\"]\n",
    "\n",
    "            # Update scores for each response (1 to 10)\n",
    "            for i in range(1, 11):\n",
    "                key = f\"scores_t5_{model_name}_{i}\"\n",
    "                for metric, metric_scores in score_data.items():\n",
    "                    if train_id in metric_scores:\n",
    "                        for sub_metric, value in metric_scores[train_id][key].items():\n",
    "                            train_data[key][sub_metric] = value\n",
    "\n",
    "            # Write updated data to the output file\n",
    "            json.dump(train_data, out_f)\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Train data updated and saved to {output_file}.\")\n",
    "\n",
    "# Process all datasets\n",
    "for model, paths in datasets.items():\n",
    "    print(f\"Processing {model} dataset...\")\n",
    "    update_train_file(\n",
    "        train_file=paths[\"train_file\"],\n",
    "        score_files=paths[\"score_files\"],\n",
    "        output_file=paths[\"output_file\"],\n",
    "        model_name=paths[\"model_name\"]\n",
    "    )\n",
    "\n",
    "print(\"All datasets successfully updated with scores.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Merge 50k Queries File with 10 Responses and Scores into Empty Response Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled dataset with average response times has been written to ../datasets/generated/step_10/train_data_with_50kx10_merged_answers_with_scores.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to files\n",
    "empty_responses_file = \"../datasets/generated/step_2/empty_response_dataset_50k.jsonl\"\n",
    "base_file_path = \"../datasets/generated/step_9/train_data_with_50kx10_t5_base_answers_with_scores.jsonl\"\n",
    "large_file_path = \"../datasets/generated/step_9/train_data_with_50kx10_t5_large_answers_with_scores.jsonl\"\n",
    "xl_file_path = \"../datasets/generated/step_9/train_data_with_50kx10_t5_xl_answers_with_scores.jsonl\"\n",
    "output_file_path = \"../datasets/generated/step_10/train_data_with_50kx10_merged_answers_with_scores.jsonl\"\n",
    "\n",
    "def load_jsonl_to_df(file_path):\n",
    "    \"\"\"\n",
    "    Loads a JSONL file into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the JSONL data.\n",
    "    \"\"\"\n",
    "    return pd.read_json(file_path, orient=\"records\", lines=True)\n",
    "\n",
    "# Load datasets into DataFrames\n",
    "empty_responses_df = load_jsonl_to_df(empty_responses_file)\n",
    "base_df = load_jsonl_to_df(base_file_path)\n",
    "large_df = load_jsonl_to_df(large_file_path)\n",
    "xl_df = load_jsonl_to_df(xl_file_path)\n",
    "\n",
    "# Check if IDs are aligned across datasets\n",
    "if not base_df[\"id\"].equals(large_df[\"id\"]) or not base_df[\"id\"].equals(xl_df[\"id\"]):\n",
    "    raise ValueError(\"ID columns do not match exactly across datasets. Please check the input files.\")\n",
    "\n",
    "# Copy ID column from base_df to empty_responses_df\n",
    "empty_responses_df[\"id\"] = base_df[\"id\"]\n",
    "\n",
    "# Copy columns for t5_base\n",
    "for i in range(1, 11):\n",
    "    empty_responses_df[f\"r_t5_base_{i}\"] = base_df[f\"r_t5_base_{i}\"]\n",
    "    empty_responses_df[f\"scores_t5_base_{i}\"] = base_df[f\"scores_t5_base_{i}\"]\n",
    "\n",
    "# Copy columns for t5_large\n",
    "for i in range(1, 11):\n",
    "    empty_responses_df[f\"r_t5_large_{i}\"] = large_df[f\"r_t5_large_{i}\"]\n",
    "    empty_responses_df[f\"scores_t5_large_{i}\"] = large_df[f\"scores_t5_large_{i}\"]\n",
    "\n",
    "# Copy columns for t5_xl\n",
    "for i in range(1, 11):\n",
    "    empty_responses_df[f\"r_t5_xl_{i}\"] = xl_df[f\"r_t5_xl_{i}\"]\n",
    "    empty_responses_df[f\"scores_t5_xl_{i}\"] = xl_df[f\"scores_t5_xl_{i}\"]\n",
    "\n",
    "# Copy average response time features\n",
    "empty_responses_df[\"rt_t5_base_avg\"] = base_df[\"rt_t5_base_avg\"]\n",
    "empty_responses_df[\"rt_t5_large_avg\"] = large_df[\"rt_t5_large_avg\"]\n",
    "empty_responses_df[\"rt_t5_xl_avg\"] = xl_df[\"rt_t5_xl_avg\"]\n",
    "\n",
    "# Save the merged dataset to a JSONL file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for entry in empty_responses_df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "print(f\"Filled dataset with average response times has been written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Compute Average Scores and Additional Response Features for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Response Features: 100%|██████| 50000/50000 [03:46<00:00, 220.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. Updated dataset saved to: ../datasets/generated/step_11/train_data_with_query_and_response_features_with_scores.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# File paths\n",
    "query_feature_file = \"../datasets/generated/step_6/train_data_with_query_features_complexity_scores.jsonl\"\n",
    "scores_file_path = \"../datasets/generated/step_10/train_data_with_50kx10_merged_answers_with_scores.jsonl\"\n",
    "output_file_path = \"../datasets/generated/step_11/train_data_with_query_and_response_features_with_scores.jsonl\"\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns its content as a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a JSON object from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "def ensure_float_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns in a DataFrame to float type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the columns to be converted.\n",
    "        columns (list): List of column names to convert to float.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "def calculate_redundancy_score(response):\n",
    "    \"\"\"\n",
    "    Calculates the redundancy score of a response based on word-level and character-level redundancy.\n",
    "\n",
    "    Args:\n",
    "        response (str): The response text to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        float: The highest redundancy score based on word-level and character-level redundancy.\n",
    "    \"\"\"\n",
    "    if not response or response.strip() == \"\":\n",
    "        return 1  # Maximum redundancy score for empty or whitespace-only responses\n",
    "\n",
    "    # Word-level redundancy\n",
    "    words = response.split()\n",
    "    word_counts = Counter(words)\n",
    "    total_words = len(words)\n",
    "    unique_words = len(word_counts)\n",
    "    word_level_redundancy = (total_words - unique_words) / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Word-char-level redundancy\n",
    "    word_char_redundancies = [\n",
    "        (len(word) - len(Counter(word))) / len(word) if len(word) > 0 else 0\n",
    "        for word in words\n",
    "    ]\n",
    "    avg_word_char_redundancy = sum(word_char_redundancies) / len(word_char_redundancies) if word_char_redundancies else 0\n",
    "\n",
    "    return max(word_level_redundancy, avg_word_char_redundancy)\n",
    "\n",
    "def is_trashy(response, redundancy_score, length, response_time):\n",
    "    \"\"\"\n",
    "    Determines whether a response should be considered as \"trashy\" based on redundancy, length, and response time.\n",
    "\n",
    "    Args:\n",
    "        response (str): The response text.\n",
    "        redundancy_score (float): The redundancy score of the response.\n",
    "        length (int): The length of the response in words.\n",
    "        response_time (float): The response generation time in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the response is considered trashy, False otherwise.\n",
    "    \"\"\"\n",
    "    max_redundancy_threshold = 0.85\n",
    "    max_length_threshold = 100\n",
    "    max_response_time_threshold = 1000\n",
    "\n",
    "    if not response.strip():\n",
    "        return True\n",
    "    elif (redundancy_score > max_redundancy_threshold and\n",
    "          length > max_length_threshold and\n",
    "          response_time > max_response_time_threshold):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def calculate_query_repetition_count(instruction, input_text, query, response):\n",
    "    \"\"\"\n",
    "    Checks whether a response is an exact repetition of the query, instruction, or input.\n",
    "\n",
    "    Args:\n",
    "        instruction (str): The instruction part of the query.\n",
    "        input_text (str): The input part of the query.\n",
    "        query (str): The full query (instruction + input).\n",
    "        response (str): The generated response.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the response is a repetition, False otherwise.\n",
    "    \"\"\"\n",
    "    if not response or response.isspace():\n",
    "        return False\n",
    "\n",
    "    # Check for exact match conditions\n",
    "    if instruction and not input_text and response.strip() == instruction.strip():\n",
    "        return True\n",
    "    if input_text and not instruction and response.strip() == input_text.strip():\n",
    "        return True\n",
    "    if instruction and input_text and response.strip() == (instruction.strip() + input_text.strip()):\n",
    "        return True\n",
    "\n",
    "    # Check if response matches the entire query\n",
    "    if response.strip() == query.strip():\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Load datasets\n",
    "query_feature_df = pd.DataFrame(read_jsonl(query_feature_file))\n",
    "scores_df = pd.DataFrame(read_jsonl(scores_file_path))\n",
    "\n",
    "# Ensure features are floats\n",
    "columns_to_convert = [\n",
    "    \"rt_t5_base_avg\", \"rt_t5_large_avg\", \"rt_t5_xl_avg\",\n",
    "    \"r_t5_base_length_avg\", \"r_t5_large_length_avg\", \"r_t5_xl_length_avg\",\n",
    "    \"r_t5_base_redundancy_avg\", \"r_t5_large_redundancy_avg\", \"r_t5_xl_redundancy_avg\"\n",
    "]\n",
    "ensure_float_columns(query_feature_df, columns_to_convert)\n",
    "\n",
    "# Process each row to calculate response features\n",
    "for index, row in tqdm(scores_df.iterrows(), total=scores_df.shape[0], desc=\"Processing Response Features\"):\n",
    "    instruction = row.get(\"instruction\", \"\")\n",
    "    input_text = row.get(\"input\", \"\")\n",
    "    query = row.get(\"query\", \"\")\n",
    "    \n",
    "    for model in [\"t5_base\", \"t5_large\", \"t5_xl\"]:\n",
    "        total_length, total_redundancy, total_scores, trash_count, repetition_count = 0, 0.0, Counter(), 0, 0\n",
    "\n",
    "        for i in range(1, 11):\n",
    "            response_key = f\"r_{model}_{i}\"\n",
    "            score_key = f\"scores_{model}_{i}\"\n",
    "\n",
    "            response = row.get(response_key, \"\")\n",
    "            response_time = row.get(f\"rt_{model}_avg\", 0.0)\n",
    "            length = len(response.split()) if response else 0\n",
    "            redundancy = calculate_redundancy_score(response)\n",
    "            total_length += length\n",
    "            total_redundancy += redundancy\n",
    "\n",
    "            scores = row.get(score_key, {})\n",
    "            for metric, value in scores.items():\n",
    "                if value is not None:\n",
    "                    total_scores[metric] += value\n",
    "\n",
    "            if is_trashy(response, redundancy, length, response_time):\n",
    "                trash_count += 1\n",
    "                \n",
    "            if calculate_query_repetition_count(instruction, input_text, query, response):\n",
    "                repetition_count += 1\n",
    "\n",
    "        avg_length = round(total_length / 10, 1)\n",
    "        avg_redundancy = round(total_redundancy / 10, 4)\n",
    "\n",
    "        avg_scores = {metric: total_scores[metric] / 10 for metric in total_scores}\n",
    "        query_feature_df.at[index, f\"rt_{model}_avg\"] = round(row.get(f\"rt_{model}_avg\", 0.0), 1)\n",
    "        query_feature_df.at[index, f\"r_{model}_length_avg\"] = avg_length\n",
    "        query_feature_df.at[index, f\"r_{model}_redundancy_avg\"] = avg_redundancy\n",
    "        query_feature_df.at[index, f\"r_{model}_trash_count\"] = trash_count\n",
    "        query_feature_df.at[index, f\"r_{model}_query_repetition_count\"] = repetition_count\n",
    "        query_feature_df.at[index, f\"scores_{model}_avg\"] = avg_scores\n",
    "        \n",
    "# Save the updated dataset\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for entry in query_feature_df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "print(\"Process completed. Updated dataset saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Calculate Discrepancies between the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric bart: Min = -9.870268821716309, Max = 0.0\n",
      "Metric rouge1: Min = 0.0, Max = 1.0\n",
      "Metric rouge2: Min = 0.0, Max = 1.0\n",
      "Metric rougeL: Min = 0.0, Max = 1.0\n",
      "Metric rougeLsum: Min = 0.0, Max = 1.0\n",
      "Metric bleu: Min = 0.0, Max = 100.00000000000003\n",
      "Metric bert: Min = -0.24503022432327198, Max = 1.000000238418579\n",
      "Metric bleurt: Min = -1.9606274127960206, Max = 1.0893199443817132\n",
      "Metric logprobs: Min = -13.237013339996338, Max = 0.0\n",
      "Process completed. Discrepancies and average normalized scores saved to: ../datasets/generated/step_12/train_data_with_query_and_response_features_discrepancies_with_scores.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Global constants\n",
    "SCORE_METRICS = ['bart', 'rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'bleu', 'bert', 'bleurt', 'logprobs']\n",
    "LLMS = ['t5_base', 't5_large', 't5_xl']\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"../datasets/generated/step_11/train_data_with_query_and_response_features_with_scores.jsonl\"\n",
    "output_file_path = \"../datasets/generated/step_12/train_data_with_query_and_response_features_discrepancies_with_scores.jsonl\"\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns its content as a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a JSON object from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def calculate_metric_ranges(df):\n",
    "    \"\"\"\n",
    "    Computes the minimum and maximum values for each score metric across all LLMs.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing score data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are metric names and values are tuples (min, max).\n",
    "    \"\"\"\n",
    "    metric_ranges = {}\n",
    "\n",
    "    for metric in SCORE_METRICS:\n",
    "        # Calculate min and max separately across each model's scores\n",
    "        min_value = min(\n",
    "            df['scores_t5_base_avg'].apply(lambda x: x.get(metric, 0)).min(),\n",
    "            df['scores_t5_large_avg'].apply(lambda x: x.get(metric, 0)).min(),\n",
    "            df['scores_t5_xl_avg'].apply(lambda x: x.get(metric, 0)).min()\n",
    "        )\n",
    "        max_value = max(\n",
    "            df['scores_t5_base_avg'].apply(lambda x: x.get(metric, 0)).max(),\n",
    "            df['scores_t5_large_avg'].apply(lambda x: x.get(metric, 0)).max(),\n",
    "            df['scores_t5_xl_avg'].apply(lambda x: x.get(metric, 0)).max()\n",
    "        )\n",
    "        metric_ranges[metric] = (min_value, max_value)\n",
    "        print(f\"Metric {metric}: Min = {min_value}, Max = {max_value}\")\n",
    "\n",
    "    return metric_ranges\n",
    "\n",
    "def normalize_metric(score, metric, metric_ranges):\n",
    "    \"\"\"\n",
    "    Normalizes a metric score using min-max scaling or log transformation.\n",
    "\n",
    "    Args:\n",
    "        score (float): The raw score value to be normalized.\n",
    "        metric (str): The name of the metric.\n",
    "        metric_ranges (dict): Dictionary containing the min and max values for each metric.\n",
    "\n",
    "    Returns:\n",
    "        float: The normalized score between 0 and 1.\n",
    "    \"\"\"\n",
    "    min_value, max_value = metric_ranges[metric]\n",
    "\n",
    "    # Define shift value for metrics, with a special shift for BLEU\n",
    "    shift_value = abs(min_value) + (2 if metric == 'bleu' else 1)\n",
    "\n",
    "    # Apply log transformation for specific high-range metrics\n",
    "    if metric in ['bart', 'bleu', 'bert', 'bleurt', 'logprobs']:\n",
    "        score = np.log(score + shift_value)\n",
    "        min_log_value = np.log(min_value + shift_value)\n",
    "        max_log_value = np.log(max_value + shift_value)\n",
    "\n",
    "        # Normalize the score between 0 and 1\n",
    "        if max_log_value == min_log_value:\n",
    "            return 0.0\n",
    "        return (score - min_log_value) / (max_log_value - min_log_value)\n",
    "    else:\n",
    "        # Direct normalization for metrics that are already between 0 and 1 (rouge scores)\n",
    "        if max_value == min_value:\n",
    "            return 0.0\n",
    "        return (score - min_value) / (max_value - min_value)\n",
    "\n",
    "def calculate_avg_normalized_scores(df, metric_ranges):\n",
    "    \"\"\"\n",
    "    Computes the average normalized score for each LLM using all metrics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing scores.\n",
    "        metric_ranges (dict): Dictionary containing the min and max values for each metric.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with new columns for average normalized scores.\n",
    "    \"\"\"\n",
    "    for llm in LLMS:\n",
    "        df[f'avg_normalized_score_{llm}'] = df.apply(\n",
    "            lambda row: round(\n",
    "                sum(\n",
    "                    normalize_metric(row[f'scores_{llm}_avg'].get(metric, 0), metric, metric_ranges)\n",
    "                    for metric in SCORE_METRICS\n",
    "                    if row[f'scores_{llm}_avg'].get(metric) is not None\n",
    "                ) / len(SCORE_METRICS),\n",
    "                4\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def calculate_discrepancies(df):\n",
    "    \"\"\"\n",
    "    Computes the discrepancy between normalized scores of different model pairs.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing average normalized scores.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with new discrepancy columns.\n",
    "    \"\"\"\n",
    "    for llm_pair in [('base', 'large'), ('large', 'xl'), ('base', 'xl')]:\n",
    "        col1, col2 = f'avg_normalized_score_t5_{llm_pair[0]}', f'avg_normalized_score_t5_{llm_pair[1]}'\n",
    "        df[f'discrepancy_{llm_pair[0]}_vs_{llm_pair[1]}'] = round(df[col1] - df[col2], 4)\n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "df = pd.DataFrame(read_jsonl(input_file_path))\n",
    "\n",
    "# Calculate metric ranges\n",
    "metric_ranges = calculate_metric_ranges(df)\n",
    "\n",
    "# Calculate average normalized scores for each LLM\n",
    "df = calculate_avg_normalized_scores(df, metric_ranges)\n",
    "\n",
    "# Calculate discrepancies\n",
    "df = calculate_discrepancies(df)\n",
    "\n",
    "# Save the updated dataset\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for entry in df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "print(\"Process completed. Discrepancies and average normalized scores saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Calculate Evaluation Model Label (Ground Truth Model Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# File paths\n",
    "# ATTENTION: Change file names according to threshold\n",
    "input_file_path = \"../datasets/generated/step_12/train_data_with_query_and_response_features_discrepancies_with_scores.jsonl\"\n",
    "output_file_path = \"../datasets/generated/step_13/train_data_with_query_and_response_features_discrepancies_with_scores_model_label_dt-0.03_tt3_rt3.jsonl\"\n",
    "log_file_path = \"../datasets/generated/step_13/model_label_distribution_t-0.03.txt\"\n",
    "\n",
    "# Thresholds\n",
    "discrepancy_threshold = -0.03 # Change to [0, -0.005, -0.01, -0.015, -0.02, -0.025, -0.03]\n",
    "repetition_count_threshold = 3\n",
    "trash_count_threshold = 3\n",
    "response_time_buffer = 1.5\n",
    "length_buffer = 1.5\n",
    "\n",
    "# Initialize counters and query ID tracking\n",
    "case_counters = defaultdict(int)\n",
    "case_query_ids = defaultdict(list)\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns its content as a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a JSON object from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def calculate_model_label(row):\n",
    "    \"\"\"\n",
    "    Determines the most suitable model (base, large, or XL) for a given query based on response quality metrics.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame containing discrepancy values, trash counts, repetition counts, and response times.\n",
    "\n",
    "    Returns:\n",
    "        int: Model label (1 = base, 2 = large, 3 = XL).\n",
    "    \"\"\"\n",
    "    query_id = row['id']\n",
    "\n",
    "    # Discrepancies\n",
    "    discrepancy_base_vs_large = row[\"discrepancy_base_vs_large\"]\n",
    "    discrepancy_large_vs_xl = row[\"discrepancy_large_vs_xl\"]\n",
    "    discrepancy_base_vs_xl = row[\"discrepancy_base_vs_xl\"]\n",
    "\n",
    "    # Trash counts\n",
    "    trash_count_base = row[\"r_t5_base_trash_count\"]\n",
    "    trash_count_large = row[\"r_t5_large_trash_count\"]\n",
    "    trash_count_xl = row[\"r_t5_xl_trash_count\"]\n",
    "\n",
    "    # Repetition counts\n",
    "    repetition_count_base = row[\"r_t5_base_query_repetition_count\"]\n",
    "    repetition_count_large = row[\"r_t5_large_query_repetition_count\"]\n",
    "    repetition_count_xl = row[\"r_t5_xl_query_repetition_count\"]\n",
    "\n",
    "    # Check 1: All models are trash\n",
    "    if trash_count_base >= trash_count_threshold and trash_count_large >= trash_count_threshold and trash_count_xl >= trash_count_threshold:\n",
    "        case_counters[\"case_1\"] += 1\n",
    "        case_query_ids[\"case_1\"].append(query_id)\n",
    "        return 3\n",
    "\n",
    "    # Check 2: XL or large is trash\n",
    "    if (trash_count_xl >= trash_count_threshold or trash_count_large >= trash_count_threshold):\n",
    "        case_counters[\"case_2\"] += 1\n",
    "        case_query_ids[\"case_2\"].append(query_id)\n",
    "        return 3\n",
    "\n",
    "    # Check 3: All discrepancies are 0 and base is not trash\n",
    "    if (discrepancy_base_vs_large == 0.0 and discrepancy_large_vs_xl == 0.0 and discrepancy_base_vs_xl == 0.0 and trash_count_base < trash_count_threshold):\n",
    "        case_counters[\"case_3\"] += 1\n",
    "        case_query_ids[\"case_3\"].append(query_id)\n",
    "        return 1\n",
    "\n",
    "    # Check 4: Smallest model outperformed all others and was not trash or query repitition\n",
    "    if (trash_count_base < trash_count_threshold and \n",
    "        discrepancy_base_vs_large >= 0 and discrepancy_base_vs_xl >= 0 and\n",
    "        repetition_count_base < repetition_count_threshold):\n",
    "        case_counters[\"case_4\"] += 1\n",
    "        case_query_ids[\"case_4\"].append(query_id)\n",
    "        return 1\n",
    "\n",
    "    # Step 6: Large outperformed all and all responses valid\n",
    "    if (trash_count_base < trash_count_threshold and trash_count_large < trash_count_threshold and trash_count_xl < trash_count_threshold and\n",
    "        discrepancy_base_vs_large < 0 and discrepancy_large_vs_xl >= 0 and repetition_count_large < repetition_count_threshold):\n",
    "        if discrepancy_base_vs_large >= discrepancy_threshold and repetition_count_base < repetition_count_threshold:\n",
    "            if (row['rt_t5_base_avg'] <= row['rt_t5_large_avg'] * response_time_buffer and\n",
    "                row['r_t5_base_length_avg'] <= row['r_t5_large_length_avg'] * length_buffer):\n",
    "                case_counters[\"case_5\"] += 1\n",
    "                case_query_ids[\"case_5\"].append(query_id)\n",
    "                return 1  # Base close to large\n",
    "        else:\n",
    "            case_counters[\"case_6\"] += 1\n",
    "            case_query_ids[\"case_6\"].append(query_id)\n",
    "            return 2  # Large selected\n",
    "\n",
    "    # Step 7: XL outperformed large and large outperformed base\n",
    "    if (trash_count_xl < trash_count_threshold and trash_count_large < trash_count_threshold and trash_count_base < trash_count_threshold and \n",
    "        discrepancy_large_vs_xl < 0 and discrepancy_base_vs_large < 0 and repetition_count_xl < repetition_count_threshold and \n",
    "        repetition_count_large < repetition_count_threshold and repetition_count_base < repetition_count_threshold):\n",
    "        if discrepancy_base_vs_xl >= discrepancy_threshold:\n",
    "            if (row['rt_t5_base_avg'] <= row['rt_t5_xl_avg'] * response_time_buffer and\n",
    "                row['r_t5_base_length_avg'] <= row['r_t5_xl_length_avg'] * length_buffer):\n",
    "                case_counters[\"case_7\"] += 1\n",
    "                case_query_ids[\"case_7\"].append(query_id)\n",
    "                return 1  # Base close to XL\n",
    "        elif discrepancy_large_vs_xl >= discrepancy_threshold:\n",
    "            if (row['rt_t5_large_avg'] <= row['rt_t5_xl_avg'] * response_time_buffer and\n",
    "                row['r_t5_large_length_avg'] <= row['r_t5_xl_length_avg'] * length_buffer):\n",
    "                case_counters[\"case_8\"] += 1\n",
    "                case_query_ids[\"case_8\"].append(query_id)\n",
    "                return 2  # Large close to XL\n",
    "\n",
    "    # Default to XL if no conditions are met\n",
    "    case_counters[\"case_default\"] += 1\n",
    "    case_query_ids[\"case_default\"].append(query_id)\n",
    "    return 3\n",
    "\n",
    "# Load dataset\n",
    "df = pd.DataFrame(read_jsonl(input_file_path))\n",
    "\n",
    "# Apply the function to each row\n",
    "df['evaluation_model_label'] = df.apply(calculate_model_label, axis=1)\n",
    "\n",
    "# Save the updated dataset\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for entry in df.to_dict(orient=\"records\"):\n",
    "        json.dump(entry, output_file, separators=(\",\", \":\"))\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "# Write case distribution to a file\n",
    "with open(log_file_path, \"w\") as report_file:\n",
    "    for case, count in case_counters.items():\n",
    "        report_file.write(f\"{case}: {count}\\n\")\n",
    "        report_file.write(f\"Query IDs: {case_query_ids[case]}\\n\\n\")\n",
    "\n",
    "    # Count occurrences of each label and calculate average complexity scores\n",
    "    label_counts = df['evaluation_model_label'].value_counts().sort_index()\n",
    "    avg_complexity_scores = df.groupby('evaluation_model_label')['query_complexity_score'].mean()\n",
    "    median_complexity_scores = df.groupby('evaluation_model_label')['query_complexity_score'].median()\n",
    "\n",
    "    report_file.write(\"\\nTotal Label Counts and Complexity Scores:\\n\")\n",
    "    for label in label_counts.index:\n",
    "        report_file.write(f\"Label {label}: {label_counts[label]}\\n\")\n",
    "        report_file.write(f\"Average complexity score: {avg_complexity_scores[label]:.4f}\\n\")\n",
    "        report_file.write(f\"Median complexity score: {median_complexity_scores[label]:.4f}\\n\\n\")\n",
    "\n",
    "# Print total occurrences and average complexity scores\n",
    "print(\"\\nLabel Counts and Complexity Scores:\")\n",
    "for label in label_counts.index:\n",
    "    print(f\"Label {label}: {label_counts[label]}\")\n",
    "    print(f\"Average complexity score: {avg_complexity_scores[label]:.4f}\")\n",
    "    print(f\"Median complexity score: {median_complexity_scores[label]:.4f}\\n\")\n",
    "\n",
    "print(f\"Processing complete. Case distribution and label counts written to {log_file_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
